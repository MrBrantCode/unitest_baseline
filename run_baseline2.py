#ç¬¬äºŒç‰ˆï¼ŒåŸºäºå·²ç»äº‹å…ˆç”Ÿæˆå¥½çš„å˜å¼‚ä½“ï¼Œç”Ÿæˆunit testè¿›è¡Œæµ‹è¯•ï¼ŒåŒå¡å¹¶è¡Œç”Ÿæˆ
import json
import os
import shutil
import subprocess
import re
import sys
import ast
import time
from tqdm import tqdm
from vllm import LLM, SamplingParams

# ================= 0. é…ç½® =================
# åŸå§‹é¢˜ç›®æ•°æ® (ç”¨äºç”Ÿæˆ Prompt)
DATA_PATH = "./datasets/CodeRM-filter/mist_test_cf.jsonl"
# é¢„ç”Ÿæˆçš„å˜å¼‚ä½“ç›®å½• (Phase 0 çš„äº§ç‰©)
MUTANT_SOURCE_DIR = "./mut_generate/mist_test_cf" 
# æ¨¡å‹è·¯å¾„
MODEL_PATH = "./model/CodeRM-8B"
# ç»“æœè¾“å‡ºç›®å½•
BASE_OUTPUT_DIR = "./mut_cp"
REPORT_DIR = "./results"
REPORT_FILE = os.path.join(REPORT_DIR, "final_report.json")

BATCH_SIZE = 10 

# ================= 1. å·¥å…·å‡½æ•° =================

def clean_output(text):
    """
    æ¸…æ´—ä»£ç ï¼šæå¤´ (Markdown/Import) + å»å°¾ (AST Truncate)
    """
    text = re.sub(r'```python', '', text)
    text = re.sub(r'```', '', text)
    
    # æå¤´
    start_marker = "import unittest"
    idx = text.find(start_marker)
    if idx != -1:
        text = text[idx:]
    else:
        class_match = re.search(r'^class\s+\w+', text, re.MULTILINE)
        if class_match:
            text = "import unittest\n" + text[class_match.start():]
    
    text = text.strip()
    
    # å»å°¾ (AST)
    try:
        tree = ast.parse(text)
        if not tree.body: return text
        
        valid_nodes = []
        for node in tree.body:
            # ç§»é™¤æ¨¡å‹è‡ªå·±å†™çš„ main å‡½æ•°
            if isinstance(node, ast.If) and isinstance(node.test, ast.Compare):
                try:
                    left = node.test.left
                    if isinstance(left, ast.Name) and left.id == "__name__":
                        continue
                except: pass
            valid_nodes.append(node)
            
        if valid_nodes:
            last_node = valid_nodes[-1]
            if hasattr(last_node, 'end_lineno'):
                end_line = last_node.end_lineno
                lines = text.split('\n')
                text = '\n'.join(lines[:end_line])
    except SyntaxError:
        pass 
    return text

def setup_task_env(task_id, solution_code, question_text, test_code, entry_point):
    """åˆå§‹åŒ–ç¯å¢ƒï¼Œå†™å…¥æºç å’Œæµ‹è¯•ä»£ç """
    task_dir = os.path.join(BASE_OUTPUT_DIR, task_id)
    os.makedirs(task_dir, exist_ok=True)
    
    # å†™å…¥ solution.py
    with open(os.path.join(task_dir, "solution.py"), "w", encoding='utf-8') as f:
        f.write(f'"""\nORIGINAL QUESTION:\n{question_text}\n"""\n\n{solution_code}')
    
    # å†™å…¥ test_suite.py
    lines = test_code.split('\n')
    cleaned_lines = [l for l in lines if not re.match(r'^from\s+solution\s+import', l.strip())]
    final_test_code = f"from solution import {entry_point}\n" + "\n".join(cleaned_lines)
    final_test_code += "\n\nif __name__ == '__main__':\n    unittest.main()\n"
    
    with open(os.path.join(task_dir, "test_suite.py"), "w", encoding='utf-8') as f:
        f.write(final_test_code)
    
    return task_dir

def load_mutants_from_disk(task_id):
    """
    ä»ç£ç›˜åŠ è½½é¢„ç”Ÿæˆçš„å˜å¼‚ä½“
    è·¯å¾„: ./mut_generate/mist_test_cf/{task_id}/mutants.json
    """
    path = os.path.join(MUTANT_SOURCE_DIR, task_id, "mutants.json")
    if not os.path.exists(path):
        return []
    
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get("mutants", [])
    except Exception as e:
        print(f"Error loading mutants for {task_id}: {e}")
        return []

def execute_test(task_dir, timeout):
    try:
        env = os.environ.copy()
        env["PYTHONPATH"] = task_dir + os.pathsep + env.get("PYTHONPATH", "")
        result = subprocess.run(
            ["python3", "test_suite.py"], cwd=task_dir, capture_output=True, text=True, env=env, timeout=timeout
        )
        combined_output = result.stderr + "\n" + result.stdout
        return result.returncode, combined_output
    except subprocess.TimeoutExpired:
        return -999, "TIMEOUT"
    except Exception as e:
        return 1, str(e)

def parse_unittest_summary(output):
    if not output: return 0, 0, 0
    run_match = re.search(r"Ran (\d+) tests", output)
    run_count = int(run_match.group(1)) if run_match else 0
    fail_count = 0
    error_count = 0
    failures_match = re.search(r"failures=(\d+)", output)
    if failures_match: fail_count = int(failures_match.group(1))
    errors_match = re.search(r"errors=(\d+)", output)
    if errors_match: error_count = int(errors_match.group(1))
    if (fail_count == 0 and error_count == 0) and "FAILED" in output: fail_count = 1 
    return run_count, fail_count, error_count

def extract_failed_tests(output):
    failed = []
    if not output: return failed
    for line in output.split('\n'):
        if line.startswith("FAIL:") or line.startswith("ERROR:"):
            parts = line.split(' ')
            if len(parts) > 1: failed.append(parts[1])
    return failed

# ================= 2. ä¸»æµç¨‹ =================

def main():
    # æ¸…ç†è¾“å‡ºç›®å½• (ä¿ç•™ REPORT_DIR)
    if os.path.exists(BASE_OUTPUT_DIR): shutil.rmtree(BASE_OUTPUT_DIR)
    os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)
    os.makedirs(REPORT_DIR, exist_ok=True)

    # æ£€æŸ¥å˜å¼‚ä½“æºç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(MUTANT_SOURCE_DIR):
        print(f"âŒ Error: Mutant source directory not found: {MUTANT_SOURCE_DIR}")
        print("Please run `generate_mut.py` first.")
        return

    # åŠ è½½é¢˜ç›®æ•°æ®
    with open(DATA_PATH, 'r') as f:
        all_data = [json.loads(line) for line in f if line.strip()]
    
    print(f"ğŸš€ Loading CodeRM-8B...")
    llm = LLM(model=MODEL_PATH, tensor_parallel_size=2, dtype="bfloat16",max_model_len=8192)
    
    sampling_params = SamplingParams(
        temperature=0.2, 
        max_tokens=4096, 
        repetition_penalty=1.05,
        stop=["<|end_of_text|>"]
    )

    final_report = []

    print(f"\nğŸš€ Starting Evaluation (Using pre-generated mutants)...")

    for i in range(0, len(all_data), BATCH_SIZE):
        batch = all_data[i : i + BATCH_SIZE]
        print(f"\n[Batch {i//BATCH_SIZE + 1}] Processing items {i} to {i+len(batch)}...")
        
        # 1. Generate Prompts
        prompts = []
        for item in batch:
            p = (f"Below is a question and it's corresponding code answer. "
                 f"Please write test cases to check the correctness of the code answer. "
                 f"You need to use the unittest library in Python and create a test class for testing.\n\n"
                 f"### question\n{item.get('original_question', '')}\n\n"
                 f"### code solution\n{item['canonical_solution']}\n\n"
                 f"Please add detailed comments.")
            prompts.append(p)
        outputs = llm.generate(prompts, sampling_params)
        
        # 2. Process
        for j, output in enumerate(outputs):
            item = batch[j]
            task_id = item['task_id']
            solution_code = item['canonical_solution']
            
            # Setup Files
            test_code = clean_output(output.outputs[0].text)
            task_dir = setup_task_env(
                task_id, solution_code, item.get('original_question', ''), test_code, item['entry_point']
            )
            
            # === Step 1: åŠ è½½é¢„ç”Ÿæˆçš„å˜å¼‚ä½“ ===
            mutants = load_mutants_from_disk(task_id)
            print(f"  Task {task_id}: Load {len(mutants)} muts. ", end="", flush=True)
            
            # === Step 2: è·‘æºç  (Source Check) ===
            # è¶…æ—¶è®¾ä¸º 60s
            ret_code, output_log = execute_test(task_dir, timeout=60)
            pass_source = (ret_code == 0)
            
            run_n, fail_n, err_n = parse_unittest_summary(output_log)
            failed_total = fail_n + err_n
            
            error_type = ""
            if ret_code == -999: error_type = "TIMEOUT"
            elif not pass_source: error_type = "ASSERT_FAIL" if run_n > 0 else "CRASH"

            source_check_info = {
                "passed": pass_source,
                "return_code": ret_code,
                "error_type": error_type,
                "stats": {"run": run_n, "failed": failed_total},
                "failed_cases": extract_failed_tests(output_log),
                "log_snippet": output_log[-500:] if output_log else ""
            }
            
            mutants_log = []
            mutation_stats = {"killed": 0, "total": len(mutants), "score": None}

            if not pass_source:
                # ç»ˆç«¯è¾“å‡ºå¤±è´¥åŸå› 
                if error_type == "TIMEOUT": print(f"âŒ Source Fail (TIMEOUT > 60s).")
                elif run_n > 0: print(f"âŒ Source Fail ({failed_total}/{run_n} failed).")
                else: print(f"âŒ Source Fail (Crash/Syntax Error).")
                
                # è®°å½•æ‰€æœ‰å˜å¼‚ä½“ä¸º SKIPPEDï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å˜å¼‚ä¿¡æ¯
                for m in mutants:
                    mutants_log.append({
                        "id": f"m_{m.get('lineno')}",
                        "operator": m.get('operator'),
                        "lineno": m.get('lineno'),
                        "original_line": m.get('original_line'), # <--- ä½ çš„éœ€æ±‚
                        "mutated_line": m.get('mutated_line'),   # <--- ä½ çš„éœ€æ±‚
                        "code": m.get('code'),                   # <--- ä½ çš„éœ€æ±‚
                        "status": "SKIPPED_SOURCE_FAIL"
                    })
            else:
                print("âœ… Source Pass.", end=" ", flush=True)
                if len(mutants) == 0:
                    print("âš ï¸ No mutants.")
                    mutation_stats["score"] = None
                else:
                    # === Step 3: è·‘å˜å¼‚ (Mutation Execution) ===
                    print("Testing mutants...", end="", flush=True)
                    killed_count = 0
                    solution_path = os.path.join(task_dir, "solution.py")
                    shutil.copy(solution_path, solution_path + ".bak")
                    
                    try:
                        for idx, m in enumerate(mutants):
                            # å†™å…¥å˜å¼‚ä»£ç 
                            with open(solution_path, "w", encoding='utf-8') as f:
                                f.write(m['code'])
                            
                            # æ‰§è¡Œæµ‹è¯• (è¶…æ—¶ 15s)
                            m_ret, _ = execute_test(task_dir, timeout=15)
                            
                            is_killed = (m_ret != 0)
                            if is_killed: killed_count += 1
                            
                            status = "KILLED" if is_killed else "SURVIVED"
                            if m_ret == -999: status = "TIMEOUT_KILLED"
                            
                            # è®°å½•è¯¦ç»†æ—¥å¿—ï¼ˆåŒ…å«æºç å’Œå˜å¼‚ä»£ç ï¼‰
                            mutants_log.append({
                                "id": f"m_{idx}",
                                "operator": m.get('operator'),
                                "lineno": m.get('lineno'),
                                "original_line": m.get('original_line'), # <--- é€ä¼ å­—æ®µ
                                "mutated_line": m.get('mutated_line'),   # <--- é€ä¼ å­—æ®µ
                                "code": m['code'],                       # <--- é€ä¼ å®Œæ•´ä»£ç 
                                "status": status,
                                "killed": is_killed
                            })
                    finally:
                        shutil.move(solution_path + ".bak", solution_path)
                    
                    mutation_stats["killed"] = killed_count
                    mutation_stats["score"] = (killed_count / len(mutants)) * 100
                    print(f"ğŸ¯ Score: {mutation_stats['score']:.1f}%")

            # ä¿å­˜ç»“æœåˆ° task ç›®å½•
            with open(os.path.join(task_dir, "mutants.json"), "w", encoding='utf-8') as f:
                json.dump({
                    "task_id": task_id, 
                    "source_check": source_check_info,
                    "mutation_stats": mutation_stats, 
                    "mutants": mutants_log
                }, f, indent=2)

            final_report.append({
                "task_id": task_id, 
                "source_check": source_check_info, 
                "mutation_check": mutation_stats
            })

        # å®æ—¶ä¿å­˜æ€»æŠ¥å‘Š
        with open(REPORT_FILE, 'w') as f: json.dump(final_report, f, indent=2)

    # æœ€ç»ˆç»Ÿè®¡
    valid_scores = [
        r['mutation_check']['score'] 
        for r in final_report 
        if r['source_check']['passed'] and r['mutation_check']['score'] is not None
    ]
    avg_score = sum(valid_scores)/len(valid_scores) if valid_scores else 0
    passed_cnt = sum(1 for r in final_report if r['source_check']['passed'])
    
    print("\n" + "="*40)
    print(f"REPORT SUMMARY")
    print(f"Pass@1: {passed_cnt}/{len(final_report)}")
    print(f"Valid Mutation Samples: {len(valid_scores)}")
    print(f"Avg Mutation Score: {avg_score:.2f}%")
    print("="*40)

if __name__ == "__main__":
    main()