{
  "task_id": "cf_38622",
  "entry_point": "compute_metrics",
  "mutant_count": 68,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):",
      "mutated_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=2, r=0):",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=2, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):",
      "mutated_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=0, r=0):",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=0, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):",
      "mutated_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=0, r=0):",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=0, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):",
      "mutated_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=-1, r=0):",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=-1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):",
      "mutated_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=1):",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=1):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):",
      "mutated_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=-1):",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=-1):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):",
      "mutated_line": "def compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=1):",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=1):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 13,
      "original_line": "precision = true_positives / (true_positives + false_positives)",
      "mutated_line": "precision = true_positives * (true_positives + false_positives)",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives * (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 13,
      "original_line": "precision = true_positives / (true_positives + false_positives)",
      "mutated_line": "precision = true_positives // (true_positives + false_positives)",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives // (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 14,
      "original_line": "recall = true_positives / (true_positives + false_negatives)",
      "mutated_line": "recall = true_positives * (true_positives + false_negatives)",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives * (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 14,
      "original_line": "recall = true_positives / (true_positives + false_negatives)",
      "mutated_line": "recall = true_positives // (true_positives + false_negatives)",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives // (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) * len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) // len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 5,
      "original_line": "metrics['acc'] = np.zeros((nmethods, nruns))",
      "mutated_line": "metrics[''] = np.zeros((nmethods, nruns))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics[''] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 6,
      "original_line": "metrics['f1'] = np.zeros((nmethods, nruns))",
      "mutated_line": "metrics[''] = np.zeros((nmethods, nruns))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics[''] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 13,
      "original_line": "precision = true_positives / (true_positives + false_positives)",
      "mutated_line": "precision = true_positives / (true_positives - false_positives)",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives - false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 13,
      "original_line": "precision = true_positives / (true_positives + false_positives)",
      "mutated_line": "precision = true_positives / (true_positives * false_positives)",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives * false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 14,
      "original_line": "recall = true_positives / (true_positives + false_negatives)",
      "mutated_line": "recall = true_positives / (true_positives - false_negatives)",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives - false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 14,
      "original_line": "recall = true_positives / (true_positives + false_negatives)",
      "mutated_line": "recall = true_positives / (true_positives * false_negatives)",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives * false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives - (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = true_positives * (len(gold_prefs) - true_positives - false_positives) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "ROR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall >= 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall >= 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "ROR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall <= 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall <= 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "ROR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) * (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) * (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) // (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) // (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 1",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 1\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else -1",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else -1\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 1",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 1\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "true_positives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j])",
      "mutated_line": "true_positives = sum((2 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((2 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "true_positives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j])",
      "mutated_line": "true_positives = sum((0 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((0 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "true_positives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j])",
      "mutated_line": "true_positives = sum((0 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((0 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "true_positives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j])",
      "mutated_line": "true_positives = sum((-1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((-1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 10,
      "original_line": "false_positives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j])",
      "mutated_line": "false_positives = sum((2 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((2 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 10,
      "original_line": "false_positives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j])",
      "mutated_line": "false_positives = sum((0 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((0 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 10,
      "original_line": "false_positives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j])",
      "mutated_line": "false_positives = sum((0 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((0 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 10,
      "original_line": "false_positives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j])",
      "mutated_line": "false_positives = sum((-1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((-1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 11,
      "original_line": "false_negatives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j])",
      "mutated_line": "false_negatives = sum((2 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((2 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 11,
      "original_line": "false_negatives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j])",
      "mutated_line": "false_negatives = sum((0 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((0 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 11,
      "original_line": "false_negatives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j])",
      "mutated_line": "false_negatives = sum((0 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((0 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 11,
      "original_line": "false_negatives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j])",
      "mutated_line": "false_negatives = sum((-1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((-1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives + false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives) * false_positives) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision - recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision - recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision * recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision * recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 1 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 1 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > -1 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > -1 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 1 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 1 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 / (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 / (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = (2 + precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = (2 + precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 ** (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 ** (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision - recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision - recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision * recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision * recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "metrics['acc'][i, r] = accuracy",
      "mutated_line": "metrics[''][i, r] = accuracy",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics[''][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "metrics['f1'][i, r] = f1_score",
      "mutated_line": "metrics[''][i, r] = f1_score",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics[''][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "ROR",
      "lineno": 9,
      "original_line": "true_positives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j])",
      "mutated_line": "true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "ROR",
      "lineno": 10,
      "original_line": "false_positives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j])",
      "mutated_line": "false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "ROR",
      "lineno": 11,
      "original_line": "false_negatives = sum(1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j])",
      "mutated_line": "false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) + true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) * true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 3 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 3 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 1 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 1 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 0 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 0 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 1 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 1 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = -2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = -2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision / recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision / recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * (precision + recall) / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * (precision + recall) / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0",
      "mutated_line": "f1_score = 2 * precision ** recall / (precision + recall) if precision + recall > 0 else 0",
      "code": "import numpy as np\n\ndef compute_metrics(nmethods, gold_prefs, predictions, metrics={}, nruns=1, r=0):\n    if not metrics:\n        metrics['acc'] = np.zeros((nmethods, nruns))\n        metrics['f1'] = np.zeros((nmethods, nruns))\n    for i in range(nmethods):\n        true_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] == predictions[i][j]))\n        false_positives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        false_negatives = sum((1 for j in range(len(gold_prefs)) if gold_prefs[j] != predictions[i][j]))\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        accuracy = (true_positives + (len(gold_prefs) - true_positives - false_positives)) / len(gold_prefs)\n        f1_score = 2 * precision ** recall / (precision + recall) if precision + recall > 0 else 0\n        metrics['acc'][i, r] = accuracy\n        metrics['f1'][i, r] = f1_score\n    return metrics"
    }
  ]
}