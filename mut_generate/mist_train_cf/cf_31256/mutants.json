{
  "task_id": "cf_31256",
  "entry_point": "cross_entropy",
  "mutant_count": 17,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "LCR",
      "lineno": 14,
      "original_line": "assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), \"Input should be NumPy arrays\"",
      "mutated_line": "assert isinstance(y, np.ndarray) or isinstance(y_, np.ndarray), 'Input should be NumPy arrays'",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) or isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), \"Input should be NumPy arrays\"",
      "mutated_line": "assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), ''",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), ''\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "ROR",
      "lineno": 15,
      "original_line": "assert y.shape == y_.shape, \"Input arrays should have the same shape\"",
      "mutated_line": "assert y.shape != y_.shape, 'Input arrays should have the same shape'",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape != y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "assert y.shape == y_.shape, \"Input arrays should have the same shape\"",
      "mutated_line": "assert y.shape == y_.shape, ''",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, ''\n    loss = -np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "UOI",
      "lineno": 18,
      "original_line": "loss = -np.sum(y_ * np.log(y))",
      "mutated_line": "loss = +np.sum(y_ * np.log(y))",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = +np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "LCR",
      "lineno": 34,
      "original_line": "assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), \"Inputs should be NumPy arrays\"",
      "mutated_line": "assert isinstance(grad_arr, np.ndarray) or isinstance(y_arr, np.ndarray) or isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) or isinstance(y_arr, np.ndarray) or isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "CRP",
      "lineno": 34,
      "original_line": "assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), \"Inputs should be NumPy arrays\"",
      "mutated_line": "assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), ''",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), ''\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "ROR",
      "lineno": 35,
      "original_line": "assert grad_arr.shape == y_arr.shape == label.shape, \"Input arrays should have the same shape\"",
      "mutated_line": "assert grad_arr.shape != y_arr.shape == label.shape, 'Input arrays should have the same shape'",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape != y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "CRP",
      "lineno": 35,
      "original_line": "assert grad_arr.shape == y_arr.shape == label.shape, \"Input arrays should have the same shape\"",
      "mutated_line": "assert grad_arr.shape == y_arr.shape == label.shape, ''",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, ''\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "AOR",
      "lineno": 38,
      "original_line": "gradient = -label / y_arr",
      "mutated_line": "np.copyto(grad_arr, gradient)",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label * y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "AOR",
      "lineno": 38,
      "original_line": "gradient = -label / y_arr",
      "mutated_line": "np.copyto(grad_arr, gradient)",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label // y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "UOI",
      "lineno": 38,
      "original_line": "gradient = -label / y_arr",
      "mutated_line": "np.copyto(grad_arr, gradient)",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ * np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = +label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "AOR",
      "lineno": 18,
      "original_line": "loss = -np.sum(y_ * np.log(y))",
      "mutated_line": "loss = -np.sum(y_ / np.log(y))",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ / np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "AOR",
      "lineno": 18,
      "original_line": "loss = -np.sum(y_ * np.log(y))",
      "mutated_line": "loss = -np.sum(y_ + np.log(y))",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ + np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    },
    {
      "operator": "AOR",
      "lineno": 18,
      "original_line": "loss = -np.sum(y_ * np.log(y))",
      "mutated_line": "loss = -np.sum(y_ ** np.log(y))",
      "code": "import numpy as np\n\ndef cross_entropy(y, y_):\n    \"\"\"\n    Calculate the cross-entropy loss between predicted and true probability distributions.\n\n    Args:\n    y (np.ndarray): Predicted probability distribution.\n    y_ (np.ndarray): True probability distribution.\n\n    Returns:\n    float: Cross-entropy loss.\n    \"\"\"\n    assert isinstance(y, np.ndarray) and isinstance(y_, np.ndarray), 'Input should be NumPy arrays'\n    assert y.shape == y_.shape, 'Input arrays should have the same shape'\n    loss = -np.sum(y_ ** np.log(y))\n    return loss\n\ndef cross_entropy_gradient(grad_arr, y_arr, label):\n    \"\"\"\n    Calculate the gradient of the cross-entropy loss with respect to the predicted probability distribution.\n\n    Args:\n    grad_arr (np.ndarray): Gradient array to store the result.\n    y_arr (np.ndarray): Predicted probability distribution.\n    label (np.ndarray): True labels.\n\n    Returns:\n    None\n    \"\"\"\n    assert isinstance(grad_arr, np.ndarray) and isinstance(y_arr, np.ndarray) and isinstance(label, np.ndarray), 'Inputs should be NumPy arrays'\n    assert grad_arr.shape == y_arr.shape == label.shape, 'Input arrays should have the same shape'\n    gradient = -label / y_arr\n    np.copyto(grad_arr, gradient)"
    }
  ]
}