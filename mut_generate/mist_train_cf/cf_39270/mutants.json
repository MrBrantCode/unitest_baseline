{
  "task_id": "cf_39270",
  "entry_point": "weibo_tokenize",
  "mutant_count": 1,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 10,
      "original_line": "word, pos_tag = token.split('/')",
      "mutated_line": "(word, pos_tag) = token.split('')",
      "code": "class WeiboToken:\n\n    def __init__(self, word, pos_tag):\n        self.word = word\n        self.pos_tag = pos_tag\n\ndef weibo_tokenize(weibo_text):\n    tokens = weibo_text.split()\n    weibo_tokens = []\n    for token in tokens:\n        (word, pos_tag) = token.split('')\n        weibo_token = WeiboToken(word, pos_tag)\n        weibo_tokens.append(weibo_token)\n    return weibo_tokens"
    }
  ]
}