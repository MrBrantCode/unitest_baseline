{
  "task_id": "cf_48799",
  "entry_point": "softmax",
  "mutant_count": 18,
  "mutants": [
    {
      "operator": "AOR",
      "lineno": 5,
      "original_line": "return e_x / e_x.sum(axis=0)",
      "mutated_line": "return e_x * e_x.sum(axis=0)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x * e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-1, 1)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "AOR",
      "lineno": 5,
      "original_line": "return e_x / e_x.sum(axis=0)",
      "mutated_line": "return e_x // e_x.sum(axis=0)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x // e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-1, 1)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "AOR",
      "lineno": 10,
      "original_line": "return np.diagflat(s) - np.dot(s, s.T)",
      "mutated_line": "return np.diagflat(s) + np.dot(s, s.T)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-1, 1)\n    return np.diagflat(s) + np.dot(s, s.T)"
    },
    {
      "operator": "AOR",
      "lineno": 10,
      "original_line": "return np.diagflat(s) - np.dot(s, s.T)",
      "mutated_line": "return np.diagflat(s) * np.dot(s, s.T)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-1, 1)\n    return np.diagflat(s) * np.dot(s, s.T)"
    },
    {
      "operator": "AOR",
      "lineno": 4,
      "original_line": "e_x = np.exp(x - np.max(x))   # subtract max value for numerical stability",
      "mutated_line": "e_x = np.exp(x + np.max(x))",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x + np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-1, 1)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "AOR",
      "lineno": 4,
      "original_line": "e_x = np.exp(x - np.max(x))   # subtract max value for numerical stability",
      "mutated_line": "e_x = np.exp(x * np.max(x))",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x * np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-1, 1)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "UOI",
      "lineno": 9,
      "original_line": "s = softmax.reshape(-1,1)",
      "mutated_line": "s = softmax.reshape(+1, 1)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(+1, 1)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "s = softmax.reshape(-1,1)",
      "mutated_line": "s = softmax.reshape(-1, 2)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-1, 2)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "s = softmax.reshape(-1,1)",
      "mutated_line": "s = softmax.reshape(-1, 0)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-1, 0)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "s = softmax.reshape(-1,1)",
      "mutated_line": "s = softmax.reshape(-1, 0)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-1, 0)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "s = softmax.reshape(-1,1)",
      "mutated_line": "s = softmax.reshape(-1, -1)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-1, -1)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "s = softmax.reshape(-1,1)",
      "mutated_line": "s = softmax.reshape(-2, 1)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-2, 1)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "s = softmax.reshape(-1,1)",
      "mutated_line": "s = softmax.reshape(-0, 1)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-0, 1)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "s = softmax.reshape(-1,1)",
      "mutated_line": "s = softmax.reshape(-0, 1)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-0, 1)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "s = softmax.reshape(-1,1)",
      "mutated_line": "s = softmax.reshape(--1, 1)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(--1, 1)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "CRP",
      "lineno": 5,
      "original_line": "return e_x / e_x.sum(axis=0)",
      "mutated_line": "return e_x / e_x.sum(axis=1)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=1)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-1, 1)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "CRP",
      "lineno": 5,
      "original_line": "return e_x / e_x.sum(axis=0)",
      "mutated_line": "return e_x / e_x.sum(axis=-1)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=-1)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-1, 1)\n    return np.diagflat(s) - np.dot(s, s.T)"
    },
    {
      "operator": "CRP",
      "lineno": 5,
      "original_line": "return e_x / e_x.sum(axis=0)",
      "mutated_line": "return e_x / e_x.sum(axis=1)",
      "code": "import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=1)\n\ndef softmax_grad(softmax):\n    s = softmax.reshape(-1, 1)\n    return np.diagflat(s) - np.dot(s, s.T)"
    }
  ]
}