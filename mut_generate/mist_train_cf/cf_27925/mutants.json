{
  "task_id": "cf_27925",
  "entry_point": "contrastive_hebbian_learning",
  "mutant_count": 25,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "eta: float = 0.1,",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=1.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=1.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "eta: float = 0.1,",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=-0.9, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=-0.9, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "eta: float = 0.1,",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "eta: float = 0.1,",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "eta: float = 0.1,",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=-0.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=-0.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "gamma: float = 0.1,",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=1.1, max_iter: int=10) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=1.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "gamma: float = 0.1,",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=-0.9, max_iter: int=10) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=-0.9, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "gamma: float = 0.1,",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0, max_iter: int=10) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "gamma: float = 0.1,",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=1, max_iter: int=10) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "gamma: float = 0.1,",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=-0.1, max_iter: int=10) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=-0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "max_iter: int = 10) -> Union[np.ndarray, list]:",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=11) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=11) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "max_iter: int = 10) -> Union[np.ndarray, list]:",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=9) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=9) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "max_iter: int = 10) -> Union[np.ndarray, list]:",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=0) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=0) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "max_iter: int = 10) -> Union[np.ndarray, list]:",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=1) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=1) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "max_iter: int = 10) -> Union[np.ndarray, list]:",
      "mutated_line": "def contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=-10) -> Union[np.ndarray, list]:",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=-10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "CRP",
      "lineno": 10,
      "original_line": "\"\"\"Contrastive Hebbian Learning.",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"\"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "ASR",
      "lineno": 29,
      "original_line": "weight += eta * (activation_difference - gamma * weight)",
      "mutated_line": "weight -= eta * (activation_difference - gamma * weight)",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight -= eta * (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "AOR",
      "lineno": 29,
      "original_line": "weight += eta * (activation_difference - gamma * weight)",
      "mutated_line": "weight += eta / (activation_difference - gamma * weight)",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta / (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "AOR",
      "lineno": 29,
      "original_line": "weight += eta * (activation_difference - gamma * weight)",
      "mutated_line": "weight += eta + (activation_difference - gamma * weight)",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta + (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "AOR",
      "lineno": 29,
      "original_line": "weight += eta * (activation_difference - gamma * weight)",
      "mutated_line": "weight += eta ** (activation_difference - gamma * weight)",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta ** (activation_difference - gamma * weight)\n    return weight"
    },
    {
      "operator": "AOR",
      "lineno": 29,
      "original_line": "weight += eta * (activation_difference - gamma * weight)",
      "mutated_line": "weight += eta * (activation_difference + gamma * weight)",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference + gamma * weight)\n    return weight"
    },
    {
      "operator": "AOR",
      "lineno": 29,
      "original_line": "weight += eta * (activation_difference - gamma * weight)",
      "mutated_line": "weight += eta * (activation_difference * (gamma * weight))",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference * (gamma * weight))\n    return weight"
    },
    {
      "operator": "AOR",
      "lineno": 29,
      "original_line": "weight += eta * (activation_difference - gamma * weight)",
      "mutated_line": "weight += eta * (activation_difference - gamma / weight)",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma / weight)\n    return weight"
    },
    {
      "operator": "AOR",
      "lineno": 29,
      "original_line": "weight += eta * (activation_difference - gamma * weight)",
      "mutated_line": "weight += eta * (activation_difference - (gamma + weight))",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - (gamma + weight))\n    return weight"
    },
    {
      "operator": "AOR",
      "lineno": 29,
      "original_line": "weight += eta * (activation_difference - gamma * weight)",
      "mutated_line": "weight += eta * (activation_difference - gamma ** weight)",
      "code": "import numpy as np\nfrom typing import Union\n\ndef contrastive_hebbian_learning(activation: Union[np.ndarray, list], weight: Union[np.ndarray, list], expected: Union[np.ndarray, list], eta: float=0.1, gamma: float=0.1, max_iter: int=10) -> Union[np.ndarray, list]:\n    \"\"\"Contrastive Hebbian Learning.\n\n    Args:\n        activation: The activation values of the neurons.\n        weight: The current weight values of the connections.\n        expected: The expected output values.\n        eta: The learning rate (default 0.1).\n        gamma: The learning rate decay factor (default 0.1).\n        max_iter: The maximum number of iterations (default 10).\n\n    Returns:\n        The updated weight after performing Contrastive Hebbian Learning.\n    \"\"\"\n    activation = np.array(activation)\n    weight = np.array(weight)\n    expected = np.array(expected)\n    for _ in range(max_iter):\n        activation_difference = np.outer(activation, expected)\n        weight += eta * (activation_difference - gamma ** weight)\n    return weight"
    }
  ]
}