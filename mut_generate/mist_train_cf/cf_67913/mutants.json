{
  "task_id": "cf_67913",
  "entry_point": "binary_cross_entropy_loss",
  "mutant_count": 33,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "epsilon = 1e-15",
      "mutated_line": "epsilon = 1.000000000000001",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1.000000000000001\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "epsilon = 1e-15",
      "mutated_line": "epsilon = -0.999999999999999",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = -0.999999999999999\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "epsilon = 1e-15",
      "mutated_line": "epsilon = 0",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 0\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "epsilon = 1e-15",
      "mutated_line": "epsilon = 1",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "epsilon = 1e-15",
      "mutated_line": "epsilon = -1e-15",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = -1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "UOI",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = +np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = +np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 20,
      "original_line": "p = np.clip(p, epsilon, 1 - epsilon)",
      "mutated_line": "p = np.clip(p, epsilon, 1 + epsilon)",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 + epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 20,
      "original_line": "p = np.clip(p, epsilon, 1 - epsilon)",
      "mutated_line": "p = np.clip(p, epsilon, 1 * epsilon)",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 * epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "p = np.clip(p, epsilon, 1 - epsilon)",
      "mutated_line": "p = np.clip(p, epsilon, 2 - epsilon)",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 2 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "p = np.clip(p, epsilon, 1 - epsilon)",
      "mutated_line": "p = np.clip(p, epsilon, 0 - epsilon)",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 0 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "p = np.clip(p, epsilon, 1 - epsilon)",
      "mutated_line": "p = np.clip(p, epsilon, 0 - epsilon)",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 0 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "p = np.clip(p, epsilon, 1 - epsilon)",
      "mutated_line": "p = np.clip(p, epsilon, -1 - epsilon)",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, -1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) - (1 - y) * np.log(1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) - (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) * ((1 - y) * np.log(1 - p)))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) * ((1 - y) * np.log(1 - p)))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y / np.log(p) + (1 - y) * np.log(1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y / np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y + np.log(p) + (1 - y) * np.log(1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y + np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y ** np.log(p) + (1 - y) * np.log(1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y ** np.log(p) + (1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (1 - y) / np.log(1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) / np.log(1 - p))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (1 - y + np.log(1 - p)))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y + np.log(1 - p)))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (1 - y) ** np.log(1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) ** np.log(1 - p))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (1 + y) * np.log(1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 + y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + 1 * y * np.log(1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + 1 * y * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (2 - y) * np.log(1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (2 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (0 - y) * np.log(1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (0 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (0 - y) * np.log(1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (0 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (-1 - y) * np.log(1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (-1 - y) * np.log(1 - p))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 + p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 + p))\n    return loss"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 * p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 * p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(2 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(2 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(0 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(0 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(0 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(0 - p))\n    return loss"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))",
      "mutated_line": "loss = -np.mean(y * np.log(p) + (1 - y) * np.log(-1 - p))",
      "code": "import numpy as np\n\ndef binary_cross_entropy_loss(y, p):\n    \"\"\"\n    Calculate the cross-entropy loss for a binary classification task.\n\n    Parameters:\n    y (numpy array): True class labels (0 or 1)\n    p (numpy array): Predicted probabilities of the positive class\n\n    Returns:\n    float: Cross-entropy loss\n    \"\"\"\n    y = np.array(y)\n    p = np.array(p)\n    epsilon = 1e-15\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(-1 - p))\n    return loss"
    }
  ]
}