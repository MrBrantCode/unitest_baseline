{
  "task_id": "cf_101025",
  "entry_point": "gradient_descent",
  "mutant_count": 127,
  "mutants": [
    {
      "operator": "AOR",
      "lineno": 3,
      "original_line": "J_history = [0.0] * num_iters",
      "mutated_line": "J_history = [0.0] / num_iters",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] / num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 3,
      "original_line": "J_history = [0.0] * num_iters",
      "mutated_line": "J_history = [0.0] + num_iters",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] + num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 3,
      "original_line": "J_history = [0.0] * num_iters",
      "mutated_line": "J_history = [0.0] ** num_iters",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] ** num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "ASR",
      "lineno": 8,
      "original_line": "theta[j] -= alpha * derivative",
      "mutated_line": "theta[j] += alpha * derivative",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] += alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) - alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) - alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) * (alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta))))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) * (alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta))))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "J_history = [0.0] * num_iters",
      "mutated_line": "J_history = [1.0] * num_iters",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [1.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "J_history = [0.0] * num_iters",
      "mutated_line": "J_history = [-1.0] * num_iters",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [-1.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "J_history = [0.0] * num_iters",
      "mutated_line": "J_history = [1] * num_iters",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [1] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m - alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m - alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m * (alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m * (alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0)))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 8,
      "original_line": "theta[j] -= alpha * derivative",
      "mutated_line": "theta[j] -= alpha / derivative",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha / derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 8,
      "original_line": "theta[j] -= alpha * derivative",
      "mutated_line": "theta[j] -= alpha + derivative",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha + derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 8,
      "original_line": "theta[j] -= alpha * derivative",
      "mutated_line": "theta[j] -= alpha ** derivative",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha ** derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) / sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) / sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) + sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) + sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = (1 / (2 * m)) ** sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = (1 / (2 * m)) ** sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) / ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) / ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + (alpha / (2 * m) + ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta))))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + (alpha / (2 * m) + ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta))))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + (alpha / (2 * m)) ** ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + (alpha / (2 * m)) ** ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) * m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) * m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) // m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) // m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m / ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m / ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + (alpha / m + ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + (alpha / m + ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0)))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + (alpha / m) ** ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + (alpha / m) ** ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 * (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 * (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 // (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 // (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha * (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha * (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha // (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha // (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) - rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) - rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) * (rho * sum((abs(t) for t in theta))))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) * (rho * sum((abs(t) for t in theta))))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 5,
      "original_line": "h = [sum(x * t for x, t in zip(row, theta)) for row in X]",
      "mutated_line": "h = [sum((x / t for (x, t) in zip(row, theta))) for row in X]",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x / t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 5,
      "original_line": "h = [sum(x * t for x, t in zip(row, theta)) for row in X]",
      "mutated_line": "h = [sum((x + t for (x, t) in zip(row, theta))) for row in X]",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x + t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 5,
      "original_line": "h = [sum(x * t for x, t in zip(row, theta)) for row in X]",
      "mutated_line": "h = [sum((x ** t for (x, t) in zip(row, theta))) for row in X]",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x ** t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha * m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha * m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha // m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha // m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] - rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] - rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] * (rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] * (rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0)))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 2 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 2 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 0 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 0 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 0 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 0 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = -1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = -1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 / m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 / m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 + m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 + m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / 2 ** m * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / 2 ** m * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 / m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 / m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 + m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 + m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / 2 ** m * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / 2 ** m * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) / sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) / sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * (1 - rho + sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * (1 - rho + sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) ** sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) ** sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho / sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho / sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + (rho + sum((abs(t) for t in theta))))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + (rho + sum((abs(t) for t in theta))))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho ** sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho ** sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) / theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) / theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * (1 - rho + theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * (1 - rho + theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) ** theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) ** theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho / (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho / (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + (rho + (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + (rho + (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0)))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho ** (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho ** (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (3 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (3 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (1 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (1 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (0 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (0 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (1 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (1 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (-2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (-2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) * 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) * 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k] + 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum((h[k] - y[k] + 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (3 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (3 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (1 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (1 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (0 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (0 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (1 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (1 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (-2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (-2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 + rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 + rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * (1 * rho * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * (1 * rho * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) / X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) / X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum((h[k] - y[k] + X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum((h[k] - y[k] + X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) ** X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) ** X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 + rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 + rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * (1 * rho * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * (1 * rho * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] + y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] + y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] * y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] * y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 3 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 3 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 1 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 1 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 0 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 0 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 1 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 1 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** -2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** -2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((2 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((2 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((0 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((0 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((0 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((0 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((-1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((-1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] + y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] + y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum((h[k] * y[k] * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum((h[k] * y[k] * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((2 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((2 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((0 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((0 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((0 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((0 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((-1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((-1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "ROR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] >= 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] >= 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "ROR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] <= 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] <= 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "ROR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] != 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] != 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (2 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (2 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (0 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (0 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (0 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (0 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (-1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (-1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t * 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t * 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t + 2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t + 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 1 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 1 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > -1 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > -1 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 1 else -1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 1 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "ROR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] <= 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] <= 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "ROR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] >= 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] >= 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "ROR",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] != 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] != 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "UOI",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else +1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else +1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 1))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 1))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else -1))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else -1))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 1))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 1))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 3 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 3 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 1 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 1 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 0 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 0 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 1 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 1 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = 1 / (2 * m) * sum((h[k] - y[k]) ** 2 for k in range(m)) + alpha / (2 * m) * ((1 - rho) * sum(t ** 2 for t in theta) + rho * sum(abs(t) for t in theta))",
      "mutated_line": "J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** -2 for t in theta)) + rho * sum((abs(t) for t in theta)))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** -2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 1 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 1 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < -1 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < -1 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 1 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -1 if theta[j] < 1 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -2 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -2 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -0 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -0 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -0 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else -0 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "derivative = sum((h[k] - y[k]) * X[k][j] for k in range(m)) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else (-1 if theta[j] < 0 else 0)))",
      "mutated_line": "derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else --1 if theta[j] < 0 else 0))",
      "code": "def gradient_descent(X, y, theta, alpha, rho, num_iters):\n    m = len(y)\n    J_history = [0.0] * num_iters\n    for i in range(num_iters):\n        h = [sum((x * t for (x, t) in zip(row, theta))) for row in X]\n        for j in range(len(theta)):\n            derivative = sum(((h[k] - y[k]) * X[k][j] for k in range(m))) / m + alpha / m * ((1 - rho) * theta[j] + rho * (1 if theta[j] > 0 else --1 if theta[j] < 0 else 0))\n            theta[j] -= alpha * derivative\n        J_history[i] = 1 / (2 * m) * sum(((h[k] - y[k]) ** 2 for k in range(m))) + alpha / (2 * m) * ((1 - rho) * sum((t ** 2 for t in theta)) + rho * sum((abs(t) for t in theta)))\n    return (theta, J_history)"
    }
  ]
}