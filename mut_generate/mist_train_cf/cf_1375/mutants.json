{
  "task_id": "cf_1375",
  "entry_point": "tokenize_sentence",
  "mutant_count": 15,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]\n    return tokenized_words"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "sentence_no_punct = re.sub(r'[^\\w\\s]', '', sentence)",
      "mutated_line": "sentence_no_punct = re.sub('', '', sentence)",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]\n    return tokenized_words"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "sentence_no_punct = re.sub(r'[^\\w\\s]', '', sentence)",
      "mutated_line": "sentence_no_punct = re.sub('[^\\\\w\\\\s]', 'MUTATED', sentence)",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', 'MUTATED', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]\n    return tokenized_words"
    },
    {
      "operator": "LCR",
      "lineno": 23,
      "original_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]",
      "mutated_line": "tokenized_words = [word for word in words if word[0].isupper() or len(word) >= 4]",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[0].isupper() or len(word) >= 4]\n    return tokenized_words"
    },
    {
      "operator": "ROR",
      "lineno": 23,
      "original_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]",
      "mutated_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) > 4]",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[0].isupper() and len(word) > 4]\n    return tokenized_words"
    },
    {
      "operator": "ROR",
      "lineno": 23,
      "original_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]",
      "mutated_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) < 4]",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[0].isupper() and len(word) < 4]\n    return tokenized_words"
    },
    {
      "operator": "ROR",
      "lineno": 23,
      "original_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]",
      "mutated_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) == 4]",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[0].isupper() and len(word) == 4]\n    return tokenized_words"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]",
      "mutated_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 5]",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 5]\n    return tokenized_words"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]",
      "mutated_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 3]",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 3]\n    return tokenized_words"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]",
      "mutated_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 0]",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 0]\n    return tokenized_words"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]",
      "mutated_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 1]",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 1]\n    return tokenized_words"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]",
      "mutated_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= -4]",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[0].isupper() and len(word) >= -4]\n    return tokenized_words"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]",
      "mutated_line": "tokenized_words = [word for word in words if word[1].isupper() and len(word) >= 4]",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[1].isupper() and len(word) >= 4]\n    return tokenized_words"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]",
      "mutated_line": "tokenized_words = [word for word in words if word[-1].isupper() and len(word) >= 4]",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[-1].isupper() and len(word) >= 4]\n    return tokenized_words"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "tokenized_words = [word for word in words if word[0].isupper() and len(word) >= 4]",
      "mutated_line": "tokenized_words = [word for word in words if word[1].isupper() and len(word) >= 4]",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize a given sentence using regular expressions.\n    \n    The function only includes words that start with a capital letter and have a length of at least 4 characters, \n    and removes any punctuation marks before tokenizing.\n\n    Args:\n        sentence (str): The input sentence to be tokenized.\n\n    Returns:\n        list: A list of tokenized words.\n    \"\"\"\n    sentence_no_punct = re.sub('[^\\\\w\\\\s]', '', sentence)\n    words = sentence_no_punct.split()\n    tokenized_words = [word for word in words if word[1].isupper() and len(word) >= 4]\n    return tokenized_words"
    }
  ]
}