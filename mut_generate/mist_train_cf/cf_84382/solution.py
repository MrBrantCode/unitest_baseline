"""
QUESTION:
Write a function called `bleu_rouge_metrics` that takes two parameters: a list of generated texts and a list of reference texts. This function should return a brief description of how BLEU and ROUGE metrics work and their differences. The function does not require any specific return type, but it should clearly state what the BLEU and ROUGE metrics measure and how they differ from each other.
"""

def bleu_rouge_metrics(generated_texts, reference_texts):
    """
    Returns a brief description of how BLEU and ROUGE metrics work and their differences.

    Parameters:
    generated_texts (list): A list of generated texts.
    reference_texts (list): A list of reference texts.

    Returns:
    str: A description of BLEU and ROUGE metrics and their differences.
    """
    description = (
        "The BLEU (Bilingual Evaluation Understudy) and ROUGE (Recall-Oriented Understudy for Giga-byte scale Text Understanding Evaluation) "
        "are two popular machine learning metrics used to evaluate the quality of text generated by models.\n"
        
        "The BLEU score focuses on precision: it measures how many n-grams (contiguous sequence of n items from a given sample of text or speech) "
        "in the generated text match any n-gram in the reference texts. However, it doesn't account for the recall (the ability of a model to find all "
        "relevant instances in the dataset), meaning it doesn't matter if some n-grams in the reference text weren't found in the generated text.\n"
        
        "Alternatively, the ROUGE metric emphasizes recall over precision. It measures how many n-grams in the reference text appear in the generated text. "
        "In addition, various types of ROUGE scores exist such as ROUGE-N, ROUGE-L, and ROUGE-S, where N focuses on n-gram stats, L considers longest "
        "matching sequence, and S considers skip-bigram which captures the sentence level structure similarity naturally.\n"
        
        "Therefore, while BLEU checks how many parts of machine translations appear in at least one of the references, ROUGE checks how much of the reference "
        "appears in the machine translations. They reflect different aspects of translation quality and might be chosen depending on specific tasks."
    )
    return description