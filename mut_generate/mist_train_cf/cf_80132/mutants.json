{
  "task_id": "cf_80132",
  "entry_point": "evaluate_classification_metrics",
  "mutant_count": 90,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 2,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "LCR",
      "lineno": 29,
      "original_line": "if true_positive_rates is not None and false_positive_rates is not None:",
      "mutated_line": "if true_positive_rates is not None or false_positive_rates is not None:",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None or false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "ROR",
      "lineno": 17,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives == 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives * (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives // (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 1\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else -1\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 1\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "ROR",
      "lineno": 20,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives == 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 20,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives * (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 20,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives // (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 1\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else -1\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 1\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "ROR",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall == 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall == 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) * (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) * (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) // (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) // (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 1",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 1\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else -1",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else -1\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 1",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 1\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "ROR",
      "lineno": 29,
      "original_line": "if true_positive_rates is not None and false_positive_rates is not None:",
      "mutated_line": "if true_positive_rates is None and false_positive_rates is not None:",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "ROR",
      "lineno": 29,
      "original_line": "if true_positive_rates is not None and false_positive_rates is not None:",
      "mutated_line": "if true_positive_rates is not None and false_positive_rates is None:",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 31,
      "original_line": "auc_roc = 0",
      "mutated_line": "auc_roc = 1",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 1\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 31,
      "original_line": "auc_roc = 0",
      "mutated_line": "auc_roc = -1",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = -1\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 31,
      "original_line": "auc_roc = 0",
      "mutated_line": "auc_roc = 1",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 1\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "ASR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc -= (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc -= (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 36,
      "original_line": "return {\"F1 score\": f1_score, \"AUC-ROC\": auc_roc}",
      "mutated_line": "return {'': f1_score, 'AUC-ROC': auc_roc}",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 36,
      "original_line": "return {\"F1 score\": f1_score, \"AUC-ROC\": auc_roc}",
      "mutated_line": "return {'F1 score': f1_score, '': auc_roc}",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, '': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives - false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives * false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 1 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != -1 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 1 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives - false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 17,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives * false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 20,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives - false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 20,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives * false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 1 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != -1 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 1 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 20,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives - false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 20,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives * false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision - recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision - recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision * recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision * recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 1 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 1 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != -1 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != -1 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 1 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 1 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 / (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 / (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = (2 + precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = (2 + precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 ** (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 ** (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision - recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision - recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision * recall) / (precision * recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision * recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 32,
      "original_line": "for i in range(1, len(true_positive_rates)):",
      "mutated_line": "for i in range(2, len(true_positive_rates)):",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(2, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 32,
      "original_line": "for i in range(1, len(true_positive_rates)):",
      "mutated_line": "for i in range(0, len(true_positive_rates)):",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(0, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 32,
      "original_line": "for i in range(1, len(true_positive_rates)):",
      "mutated_line": "for i in range(0, len(true_positive_rates)):",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(0, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 32,
      "original_line": "for i in range(1, len(true_positive_rates)):",
      "mutated_line": "for i in range(-1, len(true_positive_rates)):",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(-1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) * 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) * 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) // 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) // 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 3 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 3 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 1 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 1 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 0 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 0 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 1 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 1 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = -2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = -2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision / recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision / recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * (precision + recall) / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision + recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 23,
      "original_line": "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0",
      "mutated_line": "f1_score = 2 * precision ** recall / (precision + recall) if precision + recall != 0 else 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * precision ** recall / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) / (true_positive_rates[i - 1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) / (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1] + (true_positive_rates[i - 1] + true_positive_rates[i])) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1] + (true_positive_rates[i - 1] + true_positive_rates[i])) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) ** (true_positive_rates[i - 1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) ** (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 3",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 3\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 1",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 1\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 0",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 0\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 1",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 1\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / -2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / -2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] + false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] + false_positive_rates[i - 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += false_positive_rates[i] * false_positive_rates[i - 1] * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += false_positive_rates[i] * false_positive_rates[i - 1] * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] - true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] - true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] * true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 1] * true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i + 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i + 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i * 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i * 1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i + 1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i + 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i * 1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i * 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 2]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 2]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 0]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 0]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 0]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 0]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - -1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - -1]) * (true_positive_rates[i - 1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 2] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 2] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 0] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 0] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 0] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - 0] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    },
    {
      "operator": "CRP",
      "lineno": 33,
      "original_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i-1]) * (true_positive_rates[i-1] + true_positive_rates[i]) / 2",
      "mutated_line": "auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - -1] + true_positive_rates[i]) / 2",
      "code": "def evaluate_classification_metrics(true_positives, false_positives, false_negatives, true_positive_rates=None, false_positive_rates=None):\n    \"\"\"\n    Calculate F1 score and Area Under the Receiver Operating Characteristic curve (AUC-ROC) if thresholds are provided.\n\n    Args:\n    - true_positives (int): Number of true positives.\n    - false_positives (int): Number of false positives.\n    - false_negatives (int): Number of false negatives.\n    - true_positive_rates (list, optional): List of true positive rates for different thresholds. Defaults to None.\n    - false_positive_rates (list, optional): List of false positive rates for different thresholds. Defaults to None.\n\n    Returns:\n    - dict: Dictionary containing F1 score and AUC-ROC (if thresholds are provided).\n    \"\"\"\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    auc_roc = None\n    if true_positive_rates is not None and false_positive_rates is not None:\n        auc_roc = 0\n        for i in range(1, len(true_positive_rates)):\n            auc_roc += (false_positive_rates[i] - false_positive_rates[i - 1]) * (true_positive_rates[i - -1] + true_positive_rates[i]) / 2\n    return {'F1 score': f1_score, 'AUC-ROC': auc_roc}"
    }
  ]
}