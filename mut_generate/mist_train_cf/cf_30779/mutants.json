{
  "task_id": "cf_30779",
  "entry_point": "gradient_descent_for_function",
  "mutant_count": 38,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-6):",
      "mutated_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1001, epsilon=1e-06):",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1001, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-6):",
      "mutated_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=999, epsilon=1e-06):",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=999, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-6):",
      "mutated_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=0, epsilon=1e-06):",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=0, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-6):",
      "mutated_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1, epsilon=1e-06):",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-6):",
      "mutated_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=-1000, epsilon=1e-06):",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=-1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-6):",
      "mutated_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1.000001):",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1.000001):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-6):",
      "mutated_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=-0.999999):",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=-0.999999):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-6):",
      "mutated_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=0):",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=0):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-6):",
      "mutated_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1):",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-6):",
      "mutated_line": "def gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=-1e-06):",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=-1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x + epsilon) - f(x - epsilon)) * (2 * epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) * (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x + epsilon) - f(x - epsilon)) // (2 * epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) // (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 10,
      "original_line": "x = x - step_size * gradient",
      "mutated_line": "x = x + step_size * gradient",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x + step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 10,
      "original_line": "x = x - step_size * gradient",
      "mutated_line": "x = x * (step_size * gradient)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x * (step_size * gradient)\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "ROR",
      "lineno": 14,
      "original_line": "if abs(gradient) < epsilon:",
      "mutated_line": "if abs(gradient) <= epsilon:",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) <= epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "ROR",
      "lineno": 14,
      "original_line": "if abs(gradient) < epsilon:",
      "mutated_line": "if abs(gradient) >= epsilon:",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) >= epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "ROR",
      "lineno": 14,
      "original_line": "if abs(gradient) < epsilon:",
      "mutated_line": "if abs(gradient) != epsilon:",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) != epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "return {\"x\": x, \"f\": f(x), \"f_history\": f_history, \"x_history\": x_history}",
      "mutated_line": "return {'': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "return {\"x\": x, \"f\": f(x), \"f_history\": f_history, \"x_history\": x_history}",
      "mutated_line": "return {'x': x, '': f(x), 'f_history': f_history, 'x_history': x_history}",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, '': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "return {\"x\": x, \"f\": f(x), \"f_history\": f_history, \"x_history\": x_history}",
      "mutated_line": "return {'x': x, 'f': f(x), '': f_history, 'x_history': x_history}",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), '': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "return {\"x\": x, \"f\": f(x), \"f_history\": f_history, \"x_history\": x_history}",
      "mutated_line": "return {'x': x, 'f': f(x), 'f_history': f_history, '': x_history}",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, '': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x + epsilon) + f(x - epsilon)) / (2 * epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) + f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = f(x + epsilon) * f(x - epsilon) / (2 * epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = f(x + epsilon) * f(x - epsilon) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 / epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 / epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 + epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 + epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / 2 ** epsilon",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / 2 ** epsilon\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 10,
      "original_line": "x = x - step_size * gradient",
      "mutated_line": "x = x - step_size / gradient",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size / gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 10,
      "original_line": "x = x - step_size * gradient",
      "mutated_line": "x = x - (step_size + gradient)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - (step_size + gradient)\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 10,
      "original_line": "x = x - step_size * gradient",
      "mutated_line": "x = x - step_size ** gradient",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size ** gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (3 * epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (3 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (1 * epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (1 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (0 * epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (0 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (1 * epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (1 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (-2 * epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x - epsilon)) / (-2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x - epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x - epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x * epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x * epsilon) - f(x - epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x + epsilon) - f(x + epsilon)) / (2 * epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x + epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "gradient = (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)",
      "mutated_line": "gradient = (f(x + epsilon) - f(x * epsilon)) / (2 * epsilon)",
      "code": "import numpy as np\n\ndef gradient_descent_for_function(f, x0, step_size, max_iterations=1000, epsilon=1e-06):\n    x = x0\n    f_history = []\n    x_history = []\n    for _ in range(max_iterations):\n        gradient = (f(x + epsilon) - f(x * epsilon)) / (2 * epsilon)\n        x = x - step_size * gradient\n        f_history.append(f(x))\n        x_history.append(x)\n        if abs(gradient) < epsilon:\n            break\n    return {'x': x, 'f': f(x), 'f_history': f_history, 'x_history': x_history}"
    }
  ]
}