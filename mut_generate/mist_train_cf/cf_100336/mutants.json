{
  "task_id": "cf_100336",
  "entry_point": "adagrad",
  "mutant_count": 37,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 1,
      "original_line": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-8):",
      "mutated_line": "def adagrad(X, y, theta, alpha, num_iters, eps=1.00000001):",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1.00000001):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 1,
      "original_line": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-8):",
      "mutated_line": "def adagrad(X, y, theta, alpha, num_iters, eps=-0.99999999):",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=-0.99999999):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 1,
      "original_line": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-8):",
      "mutated_line": "def adagrad(X, y, theta, alpha, num_iters, eps=0):",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=0):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 1,
      "original_line": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-8):",
      "mutated_line": "def adagrad(X, y, theta, alpha, num_iters, eps=1):",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 1,
      "original_line": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-8):",
      "mutated_line": "def adagrad(X, y, theta, alpha, num_iters, eps=-1e-08):",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=-1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "ASR",
      "lineno": 7,
      "original_line": "G += np.square(grad)",
      "mutated_line": "G -= np.square(grad)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G -= np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "ASR",
      "lineno": 8,
      "original_line": "theta -= alpha / (np.sqrt(G) + eps) * grad",
      "mutated_line": "theta += alpha / (np.sqrt(G) + eps) * grad",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta += alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 6,
      "original_line": "grad = 1 / y.size * np.dot(X.T, h - y)",
      "mutated_line": "grad = 1 / y.size / np.dot(X.T, h - y)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size / np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 6,
      "original_line": "grad = 1 / y.size * np.dot(X.T, h - y)",
      "mutated_line": "grad = 1 / y.size + np.dot(X.T, h - y)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size + np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 6,
      "original_line": "grad = 1 / y.size * np.dot(X.T, h - y)",
      "mutated_line": "grad = (1 / y.size) ** np.dot(X.T, h - y)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = (1 / y.size) ** np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 8,
      "original_line": "theta -= alpha / (np.sqrt(G) + eps) * grad",
      "mutated_line": "theta -= alpha / (np.sqrt(G) + eps) / grad",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) / grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 8,
      "original_line": "theta -= alpha / (np.sqrt(G) + eps) * grad",
      "mutated_line": "theta -= alpha / (np.sqrt(G) + eps) + grad",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) + grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 8,
      "original_line": "theta -= alpha / (np.sqrt(G) + eps) * grad",
      "mutated_line": "theta -= (alpha / (np.sqrt(G) + eps)) ** grad",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= (alpha / (np.sqrt(G) + eps)) ** grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)",
      "mutated_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) * (2 * y.size)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) * (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)",
      "mutated_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) // (2 * y.size)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) // (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 6,
      "original_line": "grad = 1 / y.size * np.dot(X.T, h - y)",
      "mutated_line": "grad = 1 * y.size * np.dot(X.T, h - y)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 * y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 6,
      "original_line": "grad = 1 / y.size * np.dot(X.T, h - y)",
      "mutated_line": "grad = 1 // y.size * np.dot(X.T, h - y)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 // y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 8,
      "original_line": "theta -= alpha / (np.sqrt(G) + eps) * grad",
      "mutated_line": "theta -= alpha * (np.sqrt(G) + eps) * grad",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha * (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 8,
      "original_line": "theta -= alpha / (np.sqrt(G) + eps) * grad",
      "mutated_line": "theta -= alpha // (np.sqrt(G) + eps) * grad",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha // (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)",
      "mutated_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 / y.size)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 / y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)",
      "mutated_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 + y.size)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 + y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)",
      "mutated_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / 2 ** y.size",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / 2 ** y.size\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 6,
      "original_line": "grad = 1 / y.size * np.dot(X.T, h - y)",
      "mutated_line": "grad = 2 / y.size * np.dot(X.T, h - y)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 2 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 6,
      "original_line": "grad = 1 / y.size * np.dot(X.T, h - y)",
      "mutated_line": "grad = 0 / y.size * np.dot(X.T, h - y)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 0 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 6,
      "original_line": "grad = 1 / y.size * np.dot(X.T, h - y)",
      "mutated_line": "grad = 0 / y.size * np.dot(X.T, h - y)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 0 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 6,
      "original_line": "grad = 1 / y.size * np.dot(X.T, h - y)",
      "mutated_line": "grad = -1 / y.size * np.dot(X.T, h - y)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = -1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 6,
      "original_line": "grad = 1 / y.size * np.dot(X.T, h - y)",
      "mutated_line": "grad = 1 / y.size * np.dot(X.T, h + y)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h + y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 6,
      "original_line": "grad = 1 / y.size * np.dot(X.T, h - y)",
      "mutated_line": "grad = 1 / y.size * np.dot(X.T, h * y)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h * y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 8,
      "original_line": "theta -= alpha / (np.sqrt(G) + eps) * grad",
      "mutated_line": "theta -= alpha / (np.sqrt(G) - eps) * grad",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) - eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 8,
      "original_line": "theta -= alpha / (np.sqrt(G) + eps) * grad",
      "mutated_line": "theta -= alpha / (np.sqrt(G) * eps) * grad",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) * eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)",
      "mutated_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (3 * y.size)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (3 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)",
      "mutated_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (1 * y.size)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (1 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)",
      "mutated_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (0 * y.size)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (0 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)",
      "mutated_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (1 * y.size)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (1 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)",
      "mutated_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (-2 * y.size)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (-2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)",
      "mutated_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) + y)) / (2 * y.size)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) + y)) / (2 * y.size)\n    return (theta, J_history)"
    },
    {
      "operator": "AOR",
      "lineno": 9,
      "original_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) - y)) / (2 * y.size)",
      "mutated_line": "J_history[i] = np.sum(np.square(np.dot(X, theta) * y)) / (2 * y.size)",
      "code": "def adagrad(X, y, theta, alpha, num_iters, eps=1e-08):\n    J_history = np.zeros(num_iters)\n    G = np.zeros_like(theta)\n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        grad = 1 / y.size * np.dot(X.T, h - y)\n        G += np.square(grad)\n        theta -= alpha / (np.sqrt(G) + eps) * grad\n        J_history[i] = np.sum(np.square(np.dot(X, theta) * y)) / (2 * y.size)\n    return (theta, J_history)"
    }
  ]
}