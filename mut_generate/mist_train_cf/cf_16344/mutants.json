{
  "task_id": "cf_16344",
  "entry_point": "tokenize_sentence",
  "mutant_count": 4,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\"\"\"\n    sentence = re.sub('[^\\\\w\\\\s]', '', sentence)\n    tokens = re.findall('\\\\b[A-Z]\\\\w+', sentence)\n    return tokens"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "sentence = re.sub(r'[^\\w\\s]', '', sentence)",
      "mutated_line": "tokens = re.findall('\\\\b[A-Z]\\\\w+', sentence)",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize the given sentence by removing punctuation marks and \n    splitting it into words that start with a capital letter.\n    \n    Parameters:\n    sentence (str): The input sentence to be tokenized.\n    \n    Returns:\n    list: A list of words that start with a capital letter.\n    \"\"\"\n    sentence = re.sub('', '', sentence)\n    tokens = re.findall('\\\\b[A-Z]\\\\w+', sentence)\n    return tokens"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "sentence = re.sub(r'[^\\w\\s]', '', sentence)",
      "mutated_line": "tokens = re.findall('\\\\b[A-Z]\\\\w+', sentence)",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize the given sentence by removing punctuation marks and \n    splitting it into words that start with a capital letter.\n    \n    Parameters:\n    sentence (str): The input sentence to be tokenized.\n    \n    Returns:\n    list: A list of words that start with a capital letter.\n    \"\"\"\n    sentence = re.sub('[^\\\\w\\\\s]', 'MUTATED', sentence)\n    tokens = re.findall('\\\\b[A-Z]\\\\w+', sentence)\n    return tokens"
    },
    {
      "operator": "CRP",
      "lineno": 18,
      "original_line": "tokens = re.findall(r'\\b[A-Z]\\w+', sentence)",
      "mutated_line": "tokens = re.findall('', sentence)",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize the given sentence by removing punctuation marks and \n    splitting it into words that start with a capital letter.\n    \n    Parameters:\n    sentence (str): The input sentence to be tokenized.\n    \n    Returns:\n    list: A list of words that start with a capital letter.\n    \"\"\"\n    sentence = re.sub('[^\\\\w\\\\s]', '', sentence)\n    tokens = re.findall('', sentence)\n    return tokens"
    }
  ]
}