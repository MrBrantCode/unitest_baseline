{
  "task_id": "cf_79954",
  "entry_point": "manage_data_skew",
  "mutant_count": 20,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 2,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "cluster_id = 0",
      "mutated_line": "cluster_id = 1",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 1\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "cluster_id = 0",
      "mutated_line": "cluster_id = -1",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = -1\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "cluster_id = 0",
      "mutated_line": "cluster_id = 1",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 1\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 22,
      "original_line": "total_task_size = 0",
      "mutated_line": "total_task_size = 1",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 1\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 22,
      "original_line": "total_task_size = 0",
      "mutated_line": "total_task_size = -1",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = -1\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 22,
      "original_line": "total_task_size = 0",
      "mutated_line": "total_task_size = 1",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 1\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "ROR",
      "lineno": 28,
      "original_line": "if task_size > total_task_size:",
      "mutated_line": "if task_size >= total_task_size:",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size >= total_task_size:\n            cluster_id += 1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "ROR",
      "lineno": 28,
      "original_line": "if task_size > total_task_size:",
      "mutated_line": "if task_size <= total_task_size:",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size <= total_task_size:\n            cluster_id += 1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "ROR",
      "lineno": 28,
      "original_line": "if task_size > total_task_size:",
      "mutated_line": "if task_size != total_task_size:",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size != total_task_size:\n            cluster_id += 1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "ASR",
      "lineno": 29,
      "original_line": "cluster_id += 1",
      "mutated_line": "cluster_id -= 1",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id -= 1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "ASR",
      "lineno": 35,
      "original_line": "total_task_size += task_size",
      "mutated_line": "total_task_size -= task_size",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size -= task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 13,
      "original_line": "task_sizes.sort(reverse=True)",
      "mutated_line": "task_sizes.sort(reverse=False)",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=False)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 29,
      "original_line": "cluster_id += 1",
      "mutated_line": "cluster_id += 2",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 2\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 29,
      "original_line": "cluster_id += 1",
      "mutated_line": "cluster_id += 0",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 0\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 29,
      "original_line": "cluster_id += 1",
      "mutated_line": "cluster_id += 0",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 0\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 29,
      "original_line": "cluster_id += 1",
      "mutated_line": "cluster_id += -1",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += -1\n            total_task_size = 0\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 30,
      "original_line": "total_task_size = 0",
      "mutated_line": "total_task_size = 1",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 1\n            total_task_size = 1\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 30,
      "original_line": "total_task_size = 0",
      "mutated_line": "total_task_size = -1",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 1\n            total_task_size = -1\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 30,
      "original_line": "total_task_size = 0",
      "mutated_line": "total_task_size = 1",
      "code": "def manage_data_skew(task_sizes):\n    \"\"\"\n    This function optimizes data distribution among Hadoop clusters and prevents data skew.\n    \n    Parameters:\n    task_sizes (list): A list of task sizes.\n    \n    Returns:\n    dict: A dictionary where the keys are the cluster IDs and the values are lists of task sizes assigned to each cluster.\n    \"\"\"\n    task_sizes.sort(reverse=True)\n    task_distribution = {}\n    cluster_id = 0\n    total_task_size = 0\n    for task_size in task_sizes:\n        if task_size > total_task_size:\n            cluster_id += 1\n            total_task_size = 1\n            task_distribution[cluster_id] = [task_size]\n        else:\n            task_distribution.setdefault(cluster_id, []).append(task_size)\n            total_task_size += task_size\n    return task_distribution"
    }
  ]
}