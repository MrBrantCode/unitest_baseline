{
  "task_id": "cf_80122",
  "entry_point": "update_weights",
  "mutant_count": 6,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 2,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "def update_weights(learning_rate, weights, error_gradient):\n    \"\"\"\"\"\"\n    weight_updates = [weight - learning_rate * gradient for (weight, gradient) in zip(weights, error_gradient)]\n    return weight_updates"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "weight_updates = [weight - learning_rate * gradient for weight, gradient in zip(weights, error_gradient)]",
      "mutated_line": "weight_updates = [weight + learning_rate * gradient for (weight, gradient) in zip(weights, error_gradient)]",
      "code": "def update_weights(learning_rate, weights, error_gradient):\n    \"\"\"\n    Simulates one iteration of the backpropagation algorithm in a neural network \n    to update the weights and minimize the error function.\n\n    Args:\n        learning_rate (float): The learning rate for the gradient descent.\n        weights (list): The current weights of the neural network.\n        error_gradient (list): The gradient of the error function with respect to the weights.\n\n    Returns:\n        list: The updated weights.\n    \"\"\"\n    weight_updates = [weight + learning_rate * gradient for (weight, gradient) in zip(weights, error_gradient)]\n    return weight_updates"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "weight_updates = [weight - learning_rate * gradient for weight, gradient in zip(weights, error_gradient)]",
      "mutated_line": "weight_updates = [weight * (learning_rate * gradient) for (weight, gradient) in zip(weights, error_gradient)]",
      "code": "def update_weights(learning_rate, weights, error_gradient):\n    \"\"\"\n    Simulates one iteration of the backpropagation algorithm in a neural network \n    to update the weights and minimize the error function.\n\n    Args:\n        learning_rate (float): The learning rate for the gradient descent.\n        weights (list): The current weights of the neural network.\n        error_gradient (list): The gradient of the error function with respect to the weights.\n\n    Returns:\n        list: The updated weights.\n    \"\"\"\n    weight_updates = [weight * (learning_rate * gradient) for (weight, gradient) in zip(weights, error_gradient)]\n    return weight_updates"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "weight_updates = [weight - learning_rate * gradient for weight, gradient in zip(weights, error_gradient)]",
      "mutated_line": "weight_updates = [weight - learning_rate / gradient for (weight, gradient) in zip(weights, error_gradient)]",
      "code": "def update_weights(learning_rate, weights, error_gradient):\n    \"\"\"\n    Simulates one iteration of the backpropagation algorithm in a neural network \n    to update the weights and minimize the error function.\n\n    Args:\n        learning_rate (float): The learning rate for the gradient descent.\n        weights (list): The current weights of the neural network.\n        error_gradient (list): The gradient of the error function with respect to the weights.\n\n    Returns:\n        list: The updated weights.\n    \"\"\"\n    weight_updates = [weight - learning_rate / gradient for (weight, gradient) in zip(weights, error_gradient)]\n    return weight_updates"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "weight_updates = [weight - learning_rate * gradient for weight, gradient in zip(weights, error_gradient)]",
      "mutated_line": "weight_updates = [weight - (learning_rate + gradient) for (weight, gradient) in zip(weights, error_gradient)]",
      "code": "def update_weights(learning_rate, weights, error_gradient):\n    \"\"\"\n    Simulates one iteration of the backpropagation algorithm in a neural network \n    to update the weights and minimize the error function.\n\n    Args:\n        learning_rate (float): The learning rate for the gradient descent.\n        weights (list): The current weights of the neural network.\n        error_gradient (list): The gradient of the error function with respect to the weights.\n\n    Returns:\n        list: The updated weights.\n    \"\"\"\n    weight_updates = [weight - (learning_rate + gradient) for (weight, gradient) in zip(weights, error_gradient)]\n    return weight_updates"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "weight_updates = [weight - learning_rate * gradient for weight, gradient in zip(weights, error_gradient)]",
      "mutated_line": "weight_updates = [weight - learning_rate ** gradient for (weight, gradient) in zip(weights, error_gradient)]",
      "code": "def update_weights(learning_rate, weights, error_gradient):\n    \"\"\"\n    Simulates one iteration of the backpropagation algorithm in a neural network \n    to update the weights and minimize the error function.\n\n    Args:\n        learning_rate (float): The learning rate for the gradient descent.\n        weights (list): The current weights of the neural network.\n        error_gradient (list): The gradient of the error function with respect to the weights.\n\n    Returns:\n        list: The updated weights.\n    \"\"\"\n    weight_updates = [weight - learning_rate ** gradient for (weight, gradient) in zip(weights, error_gradient)]\n    return weight_updates"
    }
  ]
}