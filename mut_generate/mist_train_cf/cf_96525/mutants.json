{
  "task_id": "cf_96525",
  "entry_point": "generate_summary",
  "mutant_count": 13,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 6,
      "original_line": "sentences = re.split(r'(?<=[.!?])\\s+', text)",
      "mutated_line": "sentences = re.split('', text)",
      "code": "import re\nimport heapq\nfrom collections import Counter\n\ndef generate_summary(text, max_words):\n    sentences = re.split('', text)\n    tokens = re.findall('\\\\b\\\\w+\\\\b', text.lower())\n    stopwords = set(['the', 'and', 'in', 'etc.'])\n    tokens = [token for token in tokens if token not in stopwords]\n    word_frequency = Counter(tokens)\n    sentence_scores = []\n    for sentence in sentences:\n        sentence_tokens = [token for token in re.findall('\\\\b\\\\w+\\\\b', sentence.lower()) if token not in stopwords]\n        sentence_score = sum((word_frequency[token] for token in sentence_tokens))\n        sentence_scores.append((sentence_score, sentence))\n    top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[0])\n    summary = ' '.join((sentence for (_, sentence) in top_sentences))\n    return summary"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "tokens = re.findall(r'\\b\\w+\\b', text.lower())",
      "mutated_line": "tokens = re.findall('', text.lower())",
      "code": "import re\nimport heapq\nfrom collections import Counter\n\ndef generate_summary(text, max_words):\n    sentences = re.split('(?<=[.!?])\\\\s+', text)\n    tokens = re.findall('', text.lower())\n    stopwords = set(['the', 'and', 'in', 'etc.'])\n    tokens = [token for token in tokens if token not in stopwords]\n    word_frequency = Counter(tokens)\n    sentence_scores = []\n    for sentence in sentences:\n        sentence_tokens = [token for token in re.findall('\\\\b\\\\w+\\\\b', sentence.lower()) if token not in stopwords]\n        sentence_score = sum((word_frequency[token] for token in sentence_tokens))\n        sentence_scores.append((sentence_score, sentence))\n    top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[0])\n    summary = ' '.join((sentence for (_, sentence) in top_sentences))\n    return summary"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "stopwords = set(['the', 'and', 'in', 'etc.'])",
      "mutated_line": "stopwords = set(['', 'and', 'in', 'etc.'])",
      "code": "import re\nimport heapq\nfrom collections import Counter\n\ndef generate_summary(text, max_words):\n    sentences = re.split('(?<=[.!?])\\\\s+', text)\n    tokens = re.findall('\\\\b\\\\w+\\\\b', text.lower())\n    stopwords = set(['', 'and', 'in', 'etc.'])\n    tokens = [token for token in tokens if token not in stopwords]\n    word_frequency = Counter(tokens)\n    sentence_scores = []\n    for sentence in sentences:\n        sentence_tokens = [token for token in re.findall('\\\\b\\\\w+\\\\b', sentence.lower()) if token not in stopwords]\n        sentence_score = sum((word_frequency[token] for token in sentence_tokens))\n        sentence_scores.append((sentence_score, sentence))\n    top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[0])\n    summary = ' '.join((sentence for (_, sentence) in top_sentences))\n    return summary"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "stopwords = set(['the', 'and', 'in', 'etc.'])",
      "mutated_line": "stopwords = set(['the', '', 'in', 'etc.'])",
      "code": "import re\nimport heapq\nfrom collections import Counter\n\ndef generate_summary(text, max_words):\n    sentences = re.split('(?<=[.!?])\\\\s+', text)\n    tokens = re.findall('\\\\b\\\\w+\\\\b', text.lower())\n    stopwords = set(['the', '', 'in', 'etc.'])\n    tokens = [token for token in tokens if token not in stopwords]\n    word_frequency = Counter(tokens)\n    sentence_scores = []\n    for sentence in sentences:\n        sentence_tokens = [token for token in re.findall('\\\\b\\\\w+\\\\b', sentence.lower()) if token not in stopwords]\n        sentence_score = sum((word_frequency[token] for token in sentence_tokens))\n        sentence_scores.append((sentence_score, sentence))\n    top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[0])\n    summary = ' '.join((sentence for (_, sentence) in top_sentences))\n    return summary"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "stopwords = set(['the', 'and', 'in', 'etc.'])",
      "mutated_line": "stopwords = set(['the', 'and', '', 'etc.'])",
      "code": "import re\nimport heapq\nfrom collections import Counter\n\ndef generate_summary(text, max_words):\n    sentences = re.split('(?<=[.!?])\\\\s+', text)\n    tokens = re.findall('\\\\b\\\\w+\\\\b', text.lower())\n    stopwords = set(['the', 'and', '', 'etc.'])\n    tokens = [token for token in tokens if token not in stopwords]\n    word_frequency = Counter(tokens)\n    sentence_scores = []\n    for sentence in sentences:\n        sentence_tokens = [token for token in re.findall('\\\\b\\\\w+\\\\b', sentence.lower()) if token not in stopwords]\n        sentence_score = sum((word_frequency[token] for token in sentence_tokens))\n        sentence_scores.append((sentence_score, sentence))\n    top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[0])\n    summary = ' '.join((sentence for (_, sentence) in top_sentences))\n    return summary"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "stopwords = set(['the', 'and', 'in', 'etc.'])",
      "mutated_line": "stopwords = set(['the', 'and', 'in', ''])",
      "code": "import re\nimport heapq\nfrom collections import Counter\n\ndef generate_summary(text, max_words):\n    sentences = re.split('(?<=[.!?])\\\\s+', text)\n    tokens = re.findall('\\\\b\\\\w+\\\\b', text.lower())\n    stopwords = set(['the', 'and', 'in', ''])\n    tokens = [token for token in tokens if token not in stopwords]\n    word_frequency = Counter(tokens)\n    sentence_scores = []\n    for sentence in sentences:\n        sentence_tokens = [token for token in re.findall('\\\\b\\\\w+\\\\b', sentence.lower()) if token not in stopwords]\n        sentence_score = sum((word_frequency[token] for token in sentence_tokens))\n        sentence_scores.append((sentence_score, sentence))\n    top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[0])\n    summary = ' '.join((sentence for (_, sentence) in top_sentences))\n    return summary"
    },
    {
      "operator": "ROR",
      "lineno": 9,
      "original_line": "tokens = [token for token in tokens if token not in stopwords]",
      "mutated_line": "tokens = [token for token in tokens if token in stopwords]",
      "code": "import re\nimport heapq\nfrom collections import Counter\n\ndef generate_summary(text, max_words):\n    sentences = re.split('(?<=[.!?])\\\\s+', text)\n    tokens = re.findall('\\\\b\\\\w+\\\\b', text.lower())\n    stopwords = set(['the', 'and', 'in', 'etc.'])\n    tokens = [token for token in tokens if token in stopwords]\n    word_frequency = Counter(tokens)\n    sentence_scores = []\n    for sentence in sentences:\n        sentence_tokens = [token for token in re.findall('\\\\b\\\\w+\\\\b', sentence.lower()) if token not in stopwords]\n        sentence_score = sum((word_frequency[token] for token in sentence_tokens))\n        sentence_scores.append((sentence_score, sentence))\n    top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[0])\n    summary = ' '.join((sentence for (_, sentence) in top_sentences))\n    return summary"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "summary = ' '.join(sentence for _, sentence in top_sentences)",
      "mutated_line": "summary = ''.join((sentence for (_, sentence) in top_sentences))",
      "code": "import re\nimport heapq\nfrom collections import Counter\n\ndef generate_summary(text, max_words):\n    sentences = re.split('(?<=[.!?])\\\\s+', text)\n    tokens = re.findall('\\\\b\\\\w+\\\\b', text.lower())\n    stopwords = set(['the', 'and', 'in', 'etc.'])\n    tokens = [token for token in tokens if token not in stopwords]\n    word_frequency = Counter(tokens)\n    sentence_scores = []\n    for sentence in sentences:\n        sentence_tokens = [token for token in re.findall('\\\\b\\\\w+\\\\b', sentence.lower()) if token not in stopwords]\n        sentence_score = sum((word_frequency[token] for token in sentence_tokens))\n        sentence_scores.append((sentence_score, sentence))\n    top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[0])\n    summary = ''.join((sentence for (_, sentence) in top_sentences))\n    return summary"
    },
    {
      "operator": "ROR",
      "lineno": 13,
      "original_line": "sentence_tokens = [token for token in re.findall(r'\\b\\w+\\b', sentence.lower()) if token not in stopwords]",
      "mutated_line": "sentence_tokens = [token for token in re.findall('\\\\b\\\\w+\\\\b', sentence.lower()) if token in stopwords]",
      "code": "import re\nimport heapq\nfrom collections import Counter\n\ndef generate_summary(text, max_words):\n    sentences = re.split('(?<=[.!?])\\\\s+', text)\n    tokens = re.findall('\\\\b\\\\w+\\\\b', text.lower())\n    stopwords = set(['the', 'and', 'in', 'etc.'])\n    tokens = [token for token in tokens if token not in stopwords]\n    word_frequency = Counter(tokens)\n    sentence_scores = []\n    for sentence in sentences:\n        sentence_tokens = [token for token in re.findall('\\\\b\\\\w+\\\\b', sentence.lower()) if token in stopwords]\n        sentence_score = sum((word_frequency[token] for token in sentence_tokens))\n        sentence_scores.append((sentence_score, sentence))\n    top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[0])\n    summary = ' '.join((sentence for (_, sentence) in top_sentences))\n    return summary"
    },
    {
      "operator": "CRP",
      "lineno": 13,
      "original_line": "sentence_tokens = [token for token in re.findall(r'\\b\\w+\\b', sentence.lower()) if token not in stopwords]",
      "mutated_line": "sentence_tokens = [token for token in re.findall('', sentence.lower()) if token not in stopwords]",
      "code": "import re\nimport heapq\nfrom collections import Counter\n\ndef generate_summary(text, max_words):\n    sentences = re.split('(?<=[.!?])\\\\s+', text)\n    tokens = re.findall('\\\\b\\\\w+\\\\b', text.lower())\n    stopwords = set(['the', 'and', 'in', 'etc.'])\n    tokens = [token for token in tokens if token not in stopwords]\n    word_frequency = Counter(tokens)\n    sentence_scores = []\n    for sentence in sentences:\n        sentence_tokens = [token for token in re.findall('', sentence.lower()) if token not in stopwords]\n        sentence_score = sum((word_frequency[token] for token in sentence_tokens))\n        sentence_scores.append((sentence_score, sentence))\n    top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[0])\n    summary = ' '.join((sentence for (_, sentence) in top_sentences))\n    return summary"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[0])",
      "mutated_line": "top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[1])",
      "code": "import re\nimport heapq\nfrom collections import Counter\n\ndef generate_summary(text, max_words):\n    sentences = re.split('(?<=[.!?])\\\\s+', text)\n    tokens = re.findall('\\\\b\\\\w+\\\\b', text.lower())\n    stopwords = set(['the', 'and', 'in', 'etc.'])\n    tokens = [token for token in tokens if token not in stopwords]\n    word_frequency = Counter(tokens)\n    sentence_scores = []\n    for sentence in sentences:\n        sentence_tokens = [token for token in re.findall('\\\\b\\\\w+\\\\b', sentence.lower()) if token not in stopwords]\n        sentence_score = sum((word_frequency[token] for token in sentence_tokens))\n        sentence_scores.append((sentence_score, sentence))\n    top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[1])\n    summary = ' '.join((sentence for (_, sentence) in top_sentences))\n    return summary"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[0])",
      "mutated_line": "top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[-1])",
      "code": "import re\nimport heapq\nfrom collections import Counter\n\ndef generate_summary(text, max_words):\n    sentences = re.split('(?<=[.!?])\\\\s+', text)\n    tokens = re.findall('\\\\b\\\\w+\\\\b', text.lower())\n    stopwords = set(['the', 'and', 'in', 'etc.'])\n    tokens = [token for token in tokens if token not in stopwords]\n    word_frequency = Counter(tokens)\n    sentence_scores = []\n    for sentence in sentences:\n        sentence_tokens = [token for token in re.findall('\\\\b\\\\w+\\\\b', sentence.lower()) if token not in stopwords]\n        sentence_score = sum((word_frequency[token] for token in sentence_tokens))\n        sentence_scores.append((sentence_score, sentence))\n    top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[-1])\n    summary = ' '.join((sentence for (_, sentence) in top_sentences))\n    return summary"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[0])",
      "mutated_line": "top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[1])",
      "code": "import re\nimport heapq\nfrom collections import Counter\n\ndef generate_summary(text, max_words):\n    sentences = re.split('(?<=[.!?])\\\\s+', text)\n    tokens = re.findall('\\\\b\\\\w+\\\\b', text.lower())\n    stopwords = set(['the', 'and', 'in', 'etc.'])\n    tokens = [token for token in tokens if token not in stopwords]\n    word_frequency = Counter(tokens)\n    sentence_scores = []\n    for sentence in sentences:\n        sentence_tokens = [token for token in re.findall('\\\\b\\\\w+\\\\b', sentence.lower()) if token not in stopwords]\n        sentence_score = sum((word_frequency[token] for token in sentence_tokens))\n        sentence_scores.append((sentence_score, sentence))\n    top_sentences = heapq.nlargest(max_words, sentence_scores, key=lambda x: x[1])\n    summary = ' '.join((sentence for (_, sentence) in top_sentences))\n    return summary"
    }
  ]
}