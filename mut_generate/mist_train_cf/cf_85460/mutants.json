{
  "task_id": "cf_85460",
  "entry_point": "calculate_error",
  "mutant_count": 4,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "import numpy as np\n\ndef calculate_error(w, delta_next, activation_derivative):\n    \"\"\"\"\"\"\n    weighted_errors = np.dot(w, delta_next)\n    error = weighted_errors * activation_derivative\n    return error"
    },
    {
      "operator": "AOR",
      "lineno": 19,
      "original_line": "error = weighted_errors * activation_derivative",
      "mutated_line": "error = weighted_errors / activation_derivative",
      "code": "import numpy as np\n\ndef calculate_error(w, delta_next, activation_derivative):\n    \"\"\"\n    Calculate the error of the j-th neuron in the l-th layer of a neural network.\n\n    Parameters:\n    w (numpy array): Weights linking the current layer to the next layer.\n    delta_next (numpy array): Errors of the neurons in the next layer.\n    activation_derivative (numpy array): Derivative of the activation function evaluated at the current layer.\n\n    Returns:\n    numpy array: Error of the current layer.\n    \"\"\"\n    weighted_errors = np.dot(w, delta_next)\n    error = weighted_errors / activation_derivative\n    return error"
    },
    {
      "operator": "AOR",
      "lineno": 19,
      "original_line": "error = weighted_errors * activation_derivative",
      "mutated_line": "error = weighted_errors + activation_derivative",
      "code": "import numpy as np\n\ndef calculate_error(w, delta_next, activation_derivative):\n    \"\"\"\n    Calculate the error of the j-th neuron in the l-th layer of a neural network.\n\n    Parameters:\n    w (numpy array): Weights linking the current layer to the next layer.\n    delta_next (numpy array): Errors of the neurons in the next layer.\n    activation_derivative (numpy array): Derivative of the activation function evaluated at the current layer.\n\n    Returns:\n    numpy array: Error of the current layer.\n    \"\"\"\n    weighted_errors = np.dot(w, delta_next)\n    error = weighted_errors + activation_derivative\n    return error"
    },
    {
      "operator": "AOR",
      "lineno": 19,
      "original_line": "error = weighted_errors * activation_derivative",
      "mutated_line": "error = weighted_errors ** activation_derivative",
      "code": "import numpy as np\n\ndef calculate_error(w, delta_next, activation_derivative):\n    \"\"\"\n    Calculate the error of the j-th neuron in the l-th layer of a neural network.\n\n    Parameters:\n    w (numpy array): Weights linking the current layer to the next layer.\n    delta_next (numpy array): Errors of the neurons in the next layer.\n    activation_derivative (numpy array): Derivative of the activation function evaluated at the current layer.\n\n    Returns:\n    numpy array: Error of the current layer.\n    \"\"\"\n    weighted_errors = np.dot(w, delta_next)\n    error = weighted_errors ** activation_derivative\n    return error"
    }
  ]
}