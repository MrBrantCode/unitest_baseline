{
  "task_id": "cf_34867",
  "entry_point": "finite_difference_gradient",
  "mutant_count": 14,
  "mutants": [
    {
      "operator": "ASR",
      "lineno": 9,
      "original_line": "x_plus_h[i] += h",
      "mutated_line": "x_plus_h[i] -= h",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] -= h\n        x_minus_h = x.copy()\n        x_minus_h[i] -= h\n        grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)\n    return grad"
    },
    {
      "operator": "ASR",
      "lineno": 11,
      "original_line": "x_minus_h[i] -= h",
      "mutated_line": "x_minus_h[i] += h",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] += h\n        x_minus_h = x.copy()\n        x_minus_h[i] += h\n        grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)\n    return grad"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)",
      "mutated_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) * (2 * h)",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] += h\n        x_minus_h = x.copy()\n        x_minus_h[i] -= h\n        grad[i] = (f(x_plus_h) - f(x_minus_h)) * (2 * h)\n    return grad"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)",
      "mutated_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) // (2 * h)",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] += h\n        x_minus_h = x.copy()\n        x_minus_h[i] -= h\n        grad[i] = (f(x_plus_h) - f(x_minus_h)) // (2 * h)\n    return grad"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)",
      "mutated_line": "grad[i] = (f(x_plus_h) + f(x_minus_h)) / (2 * h)",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] += h\n        x_minus_h = x.copy()\n        x_minus_h[i] -= h\n        grad[i] = (f(x_plus_h) + f(x_minus_h)) / (2 * h)\n    return grad"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)",
      "mutated_line": "grad[i] = f(x_plus_h) * f(x_minus_h) / (2 * h)",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] += h\n        x_minus_h = x.copy()\n        x_minus_h[i] -= h\n        grad[i] = f(x_plus_h) * f(x_minus_h) / (2 * h)\n    return grad"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)",
      "mutated_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 / h)",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] += h\n        x_minus_h = x.copy()\n        x_minus_h[i] -= h\n        grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 / h)\n    return grad"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)",
      "mutated_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 + h)",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] += h\n        x_minus_h = x.copy()\n        x_minus_h[i] -= h\n        grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 + h)\n    return grad"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)",
      "mutated_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / 2 ** h",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] += h\n        x_minus_h = x.copy()\n        x_minus_h[i] -= h\n        grad[i] = (f(x_plus_h) - f(x_minus_h)) / 2 ** h\n    return grad"
    },
    {
      "operator": "CRP",
      "lineno": 12,
      "original_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)",
      "mutated_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (3 * h)",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] += h\n        x_minus_h = x.copy()\n        x_minus_h[i] -= h\n        grad[i] = (f(x_plus_h) - f(x_minus_h)) / (3 * h)\n    return grad"
    },
    {
      "operator": "CRP",
      "lineno": 12,
      "original_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)",
      "mutated_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (1 * h)",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] += h\n        x_minus_h = x.copy()\n        x_minus_h[i] -= h\n        grad[i] = (f(x_plus_h) - f(x_minus_h)) / (1 * h)\n    return grad"
    },
    {
      "operator": "CRP",
      "lineno": 12,
      "original_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)",
      "mutated_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (0 * h)",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] += h\n        x_minus_h = x.copy()\n        x_minus_h[i] -= h\n        grad[i] = (f(x_plus_h) - f(x_minus_h)) / (0 * h)\n    return grad"
    },
    {
      "operator": "CRP",
      "lineno": 12,
      "original_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)",
      "mutated_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (1 * h)",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] += h\n        x_minus_h = x.copy()\n        x_minus_h[i] -= h\n        grad[i] = (f(x_plus_h) - f(x_minus_h)) / (1 * h)\n    return grad"
    },
    {
      "operator": "CRP",
      "lineno": 12,
      "original_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (2 * h)",
      "mutated_line": "grad[i] = (f(x_plus_h) - f(x_minus_h)) / (-2 * h)",
      "code": "import numpy as np\n\ndef finite_difference_gradient(f, x, h):\n    (n_params, n_train) = x.shape\n    grad = np.zeros((n_params, n_train))\n    for i in range(n_params):\n        x_plus_h = x.copy()\n        x_plus_h[i] += h\n        x_minus_h = x.copy()\n        x_minus_h[i] -= h\n        grad[i] = (f(x_plus_h) - f(x_minus_h)) / (-2 * h)\n    return grad"
    }
  ]
}