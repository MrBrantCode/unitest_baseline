{
  "task_id": "cf_66513",
  "entry_point": "gradient_f_w",
  "mutant_count": 8,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "import numpy as np\n\ndef gradient_f_w(X, y, w, lambda_val):\n    \"\"\"\"\"\"\n    return np.dot(X.T, np.dot(X, w) - y) + lambda_val * w"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "return np.dot(X.T, np.dot(X, w) - y) + lambda_val * w",
      "mutated_line": "return np.dot(X.T, np.dot(X, w) - y) - lambda_val * w",
      "code": "import numpy as np\n\ndef gradient_f_w(X, y, w, lambda_val):\n    \"\"\"\n    Calculate the gradient of the regularized linear regression objective with respect to w.\n\n    Args:\n    X (numpy array): Design matrix\n    y (numpy array): Target vector\n    w (numpy array): Weight vector\n    lambda_val (float): Regularization parameter\n\n    Returns:\n    numpy array: Gradient of the objective function with respect to w\n    \"\"\"\n    return np.dot(X.T, np.dot(X, w) - y) - lambda_val * w"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "return np.dot(X.T, np.dot(X, w) - y) + lambda_val * w",
      "mutated_line": "return np.dot(X.T, np.dot(X, w) - y) * (lambda_val * w)",
      "code": "import numpy as np\n\ndef gradient_f_w(X, y, w, lambda_val):\n    \"\"\"\n    Calculate the gradient of the regularized linear regression objective with respect to w.\n\n    Args:\n    X (numpy array): Design matrix\n    y (numpy array): Target vector\n    w (numpy array): Weight vector\n    lambda_val (float): Regularization parameter\n\n    Returns:\n    numpy array: Gradient of the objective function with respect to w\n    \"\"\"\n    return np.dot(X.T, np.dot(X, w) - y) * (lambda_val * w)"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "return np.dot(X.T, np.dot(X, w) - y) + lambda_val * w",
      "mutated_line": "return np.dot(X.T, np.dot(X, w) - y) + lambda_val / w",
      "code": "import numpy as np\n\ndef gradient_f_w(X, y, w, lambda_val):\n    \"\"\"\n    Calculate the gradient of the regularized linear regression objective with respect to w.\n\n    Args:\n    X (numpy array): Design matrix\n    y (numpy array): Target vector\n    w (numpy array): Weight vector\n    lambda_val (float): Regularization parameter\n\n    Returns:\n    numpy array: Gradient of the objective function with respect to w\n    \"\"\"\n    return np.dot(X.T, np.dot(X, w) - y) + lambda_val / w"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "return np.dot(X.T, np.dot(X, w) - y) + lambda_val * w",
      "mutated_line": "return np.dot(X.T, np.dot(X, w) - y) + (lambda_val + w)",
      "code": "import numpy as np\n\ndef gradient_f_w(X, y, w, lambda_val):\n    \"\"\"\n    Calculate the gradient of the regularized linear regression objective with respect to w.\n\n    Args:\n    X (numpy array): Design matrix\n    y (numpy array): Target vector\n    w (numpy array): Weight vector\n    lambda_val (float): Regularization parameter\n\n    Returns:\n    numpy array: Gradient of the objective function with respect to w\n    \"\"\"\n    return np.dot(X.T, np.dot(X, w) - y) + (lambda_val + w)"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "return np.dot(X.T, np.dot(X, w) - y) + lambda_val * w",
      "mutated_line": "return np.dot(X.T, np.dot(X, w) - y) + lambda_val ** w",
      "code": "import numpy as np\n\ndef gradient_f_w(X, y, w, lambda_val):\n    \"\"\"\n    Calculate the gradient of the regularized linear regression objective with respect to w.\n\n    Args:\n    X (numpy array): Design matrix\n    y (numpy array): Target vector\n    w (numpy array): Weight vector\n    lambda_val (float): Regularization parameter\n\n    Returns:\n    numpy array: Gradient of the objective function with respect to w\n    \"\"\"\n    return np.dot(X.T, np.dot(X, w) - y) + lambda_val ** w"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "return np.dot(X.T, np.dot(X, w) - y) + lambda_val * w",
      "mutated_line": "return np.dot(X.T, np.dot(X, w) + y) + lambda_val * w",
      "code": "import numpy as np\n\ndef gradient_f_w(X, y, w, lambda_val):\n    \"\"\"\n    Calculate the gradient of the regularized linear regression objective with respect to w.\n\n    Args:\n    X (numpy array): Design matrix\n    y (numpy array): Target vector\n    w (numpy array): Weight vector\n    lambda_val (float): Regularization parameter\n\n    Returns:\n    numpy array: Gradient of the objective function with respect to w\n    \"\"\"\n    return np.dot(X.T, np.dot(X, w) + y) + lambda_val * w"
    },
    {
      "operator": "AOR",
      "lineno": 16,
      "original_line": "return np.dot(X.T, np.dot(X, w) - y) + lambda_val * w",
      "mutated_line": "return np.dot(X.T, np.dot(X, w) * y) + lambda_val * w",
      "code": "import numpy as np\n\ndef gradient_f_w(X, y, w, lambda_val):\n    \"\"\"\n    Calculate the gradient of the regularized linear regression objective with respect to w.\n\n    Args:\n    X (numpy array): Design matrix\n    y (numpy array): Target vector\n    w (numpy array): Weight vector\n    lambda_val (float): Regularization parameter\n\n    Returns:\n    numpy array: Gradient of the objective function with respect to w\n    \"\"\"\n    return np.dot(X.T, np.dot(X, w) * y) + lambda_val * w"
    }
  ]
}