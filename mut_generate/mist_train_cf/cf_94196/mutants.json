{
  "task_id": "cf_94196",
  "entry_point": "tokenize_sentence",
  "mutant_count": 4,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\"\"\"\n    sentence = re.sub('[^\\\\w\\\\s]', '', sentence)\n    tokens = re.findall('\\\\b[A-Z]\\\\w+', sentence)\n    return tokens"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "sentence = re.sub(r'[^\\w\\s]', '', sentence)",
      "mutated_line": "tokens = re.findall('\\\\b[A-Z]\\\\w+', sentence)",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize the given sentence into words starting with a capital letter.\n\n    Args:\n    sentence (str): The input sentence.\n\n    Returns:\n    list: A list of tokenized words.\n    \"\"\"\n    sentence = re.sub('', '', sentence)\n    tokens = re.findall('\\\\b[A-Z]\\\\w+', sentence)\n    return tokens"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "sentence = re.sub(r'[^\\w\\s]', '', sentence)",
      "mutated_line": "tokens = re.findall('\\\\b[A-Z]\\\\w+', sentence)",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize the given sentence into words starting with a capital letter.\n\n    Args:\n    sentence (str): The input sentence.\n\n    Returns:\n    list: A list of tokenized words.\n    \"\"\"\n    sentence = re.sub('[^\\\\w\\\\s]', 'MUTATED', sentence)\n    tokens = re.findall('\\\\b[A-Z]\\\\w+', sentence)\n    return tokens"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "tokens = re.findall(r'\\b[A-Z]\\w+', sentence)",
      "mutated_line": "tokens = re.findall('', sentence)",
      "code": "import re\n\ndef tokenize_sentence(sentence):\n    \"\"\"\n    Tokenize the given sentence into words starting with a capital letter.\n\n    Args:\n    sentence (str): The input sentence.\n\n    Returns:\n    list: A list of tokenized words.\n    \"\"\"\n    sentence = re.sub('[^\\\\w\\\\s]', '', sentence)\n    tokens = re.findall('', sentence)\n    return tokens"
    }
  ]
}