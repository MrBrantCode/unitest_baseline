{
  "task_id": "cf_43280",
  "entry_point": "proximal_gradient_descent",
  "mutant_count": 33,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):",
      "mutated_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=False, alpha=0.3, beta=0.5):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=False, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):",
      "mutated_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=1.3, beta=0.5):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=1.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):",
      "mutated_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=-0.7, beta=0.5):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=-0.7, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):",
      "mutated_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0, beta=0.5):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):",
      "mutated_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=1, beta=0.5):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=1, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):",
      "mutated_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=-0.3, beta=0.5):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=-0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):",
      "mutated_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=1.5):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=1.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):",
      "mutated_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=-0.5):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=-0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):",
      "mutated_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):",
      "mutated_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=1):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=1):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):",
      "mutated_line": "def proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=-0.5):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=-0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "tau = 1.0",
      "mutated_line": "tau = 2.0",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 2.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "tau = 1.0",
      "mutated_line": "tau = 0.0",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 0.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "tau = 1.0",
      "mutated_line": "tau = 0",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "tau = 1.0",
      "mutated_line": "tau = -1.0",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = -1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "CRP",
      "lineno": 5,
      "original_line": "while True:",
      "mutated_line": "while False:",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while False:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "AOR",
      "lineno": 6,
      "original_line": "x_hat = x - tau * gradfx",
      "mutated_line": "x_hat = x + tau * gradfx",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x + tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "AOR",
      "lineno": 6,
      "original_line": "x_hat = x - tau * gradfx",
      "mutated_line": "x_hat = x * (tau * gradfx)",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x * (tau * gradfx)\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "ROR",
      "lineno": 12,
      "original_line": "if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "mutated_line": "if fz < f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz < f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "ROR",
      "lineno": 12,
      "original_line": "if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "mutated_line": "if fz > f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz > f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "ROR",
      "lineno": 12,
      "original_line": "if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "mutated_line": "if fz == f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz == f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "ASR",
      "lineno": 16,
      "original_line": "tau *= beta",
      "mutated_line": "tau /= beta",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau /= beta\n    return x"
    },
    {
      "operator": "AOR",
      "lineno": 6,
      "original_line": "x_hat = x - tau * gradfx",
      "mutated_line": "x_hat = x - tau / gradfx",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau / gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "AOR",
      "lineno": 6,
      "original_line": "x_hat = x - tau * gradfx",
      "mutated_line": "x_hat = x - (tau + gradfx)",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - (tau + gradfx)\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "AOR",
      "lineno": 6,
      "original_line": "x_hat = x - tau * gradfx",
      "mutated_line": "x_hat = x - tau ** gradfx",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau ** gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "mutated_line": "if fz <= f(x) + alpha * tau * np.dot(gradfx, gradfx):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) + alpha * tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "mutated_line": "if fz <= f(x) * (alpha * tau * np.dot(gradfx, gradfx)):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) * (alpha * tau * np.dot(gradfx, gradfx)):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "mutated_line": "if fz <= f(x) - alpha * tau / np.dot(gradfx, gradfx):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha * tau / np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "mutated_line": "if fz <= f(x) - (alpha * tau + np.dot(gradfx, gradfx)):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - (alpha * tau + np.dot(gradfx, gradfx)):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "mutated_line": "if fz <= f(x) - (alpha * tau) ** np.dot(gradfx, gradfx):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - (alpha * tau) ** np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "mutated_line": "if fz <= f(x) - alpha / tau * np.dot(gradfx, gradfx):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha / tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "mutated_line": "if fz <= f(x) - (alpha + tau) * np.dot(gradfx, gradfx):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - (alpha + tau) * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    },
    {
      "operator": "AOR",
      "lineno": 12,
      "original_line": "if fz <= f(x) - alpha * tau * np.dot(gradfx, gradfx):",
      "mutated_line": "if fz <= f(x) - alpha ** tau * np.dot(gradfx, gradfx):",
      "code": "import numpy as np\n\ndef proximal_gradient_descent(x, gradfx, prox, f, linesearch=True, alpha=0.3, beta=0.5):\n    tau = 1.0\n    while True:\n        x_hat = x - tau * gradfx\n        z = prox(x_hat, tau)\n        fz = f(z)\n        if not linesearch:\n            x = z\n            break\n        if fz <= f(x) - alpha ** tau * np.dot(gradfx, gradfx):\n            x = z\n            break\n        else:\n            tau *= beta\n    return x"
    }
  ]
}