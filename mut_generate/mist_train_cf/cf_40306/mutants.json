{
  "task_id": "cf_40306",
  "entry_point": "tokenize_string",
  "mutant_count": 11,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "start = 0",
      "mutated_line": "start = 1",
      "code": "def tokenize_string(input_string, keywords):\n    tokens = []\n    for keyword in keywords:\n        start = 1\n        while True:\n            start = input_string.find(keyword, start)\n            if start == -1:\n                break\n            tokens.append((keyword, start))\n            start += len(keyword)\n    return tokens"
    },
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "start = 0",
      "mutated_line": "start = -1",
      "code": "def tokenize_string(input_string, keywords):\n    tokens = []\n    for keyword in keywords:\n        start = -1\n        while True:\n            start = input_string.find(keyword, start)\n            if start == -1:\n                break\n            tokens.append((keyword, start))\n            start += len(keyword)\n    return tokens"
    },
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "start = 0",
      "mutated_line": "start = 1",
      "code": "def tokenize_string(input_string, keywords):\n    tokens = []\n    for keyword in keywords:\n        start = 1\n        while True:\n            start = input_string.find(keyword, start)\n            if start == -1:\n                break\n            tokens.append((keyword, start))\n            start += len(keyword)\n    return tokens"
    },
    {
      "operator": "CRP",
      "lineno": 5,
      "original_line": "while True:",
      "mutated_line": "while False:",
      "code": "def tokenize_string(input_string, keywords):\n    tokens = []\n    for keyword in keywords:\n        start = 0\n        while False:\n            start = input_string.find(keyword, start)\n            if start == -1:\n                break\n            tokens.append((keyword, start))\n            start += len(keyword)\n    return tokens"
    },
    {
      "operator": "ASR",
      "lineno": 10,
      "original_line": "start += len(keyword)",
      "mutated_line": "start -= len(keyword)",
      "code": "def tokenize_string(input_string, keywords):\n    tokens = []\n    for keyword in keywords:\n        start = 0\n        while True:\n            start = input_string.find(keyword, start)\n            if start == -1:\n                break\n            tokens.append((keyword, start))\n            start -= len(keyword)\n    return tokens"
    },
    {
      "operator": "ROR",
      "lineno": 7,
      "original_line": "if start == -1:",
      "mutated_line": "if start != -1:",
      "code": "def tokenize_string(input_string, keywords):\n    tokens = []\n    for keyword in keywords:\n        start = 0\n        while True:\n            start = input_string.find(keyword, start)\n            if start != -1:\n                break\n            tokens.append((keyword, start))\n            start += len(keyword)\n    return tokens"
    },
    {
      "operator": "UOI",
      "lineno": 7,
      "original_line": "if start == -1:",
      "mutated_line": "if start == +1:",
      "code": "def tokenize_string(input_string, keywords):\n    tokens = []\n    for keyword in keywords:\n        start = 0\n        while True:\n            start = input_string.find(keyword, start)\n            if start == +1:\n                break\n            tokens.append((keyword, start))\n            start += len(keyword)\n    return tokens"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "if start == -1:",
      "mutated_line": "if start == -2:",
      "code": "def tokenize_string(input_string, keywords):\n    tokens = []\n    for keyword in keywords:\n        start = 0\n        while True:\n            start = input_string.find(keyword, start)\n            if start == -2:\n                break\n            tokens.append((keyword, start))\n            start += len(keyword)\n    return tokens"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "if start == -1:",
      "mutated_line": "if start == -0:",
      "code": "def tokenize_string(input_string, keywords):\n    tokens = []\n    for keyword in keywords:\n        start = 0\n        while True:\n            start = input_string.find(keyword, start)\n            if start == -0:\n                break\n            tokens.append((keyword, start))\n            start += len(keyword)\n    return tokens"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "if start == -1:",
      "mutated_line": "if start == -0:",
      "code": "def tokenize_string(input_string, keywords):\n    tokens = []\n    for keyword in keywords:\n        start = 0\n        while True:\n            start = input_string.find(keyword, start)\n            if start == -0:\n                break\n            tokens.append((keyword, start))\n            start += len(keyword)\n    return tokens"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "if start == -1:",
      "mutated_line": "if start == --1:",
      "code": "def tokenize_string(input_string, keywords):\n    tokens = []\n    for keyword in keywords:\n        start = 0\n        while True:\n            start = input_string.find(keyword, start)\n            if start == --1:\n                break\n            tokens.append((keyword, start))\n            start += len(keyword)\n    return tokens"
    }
  ]
}