{
  "task_id": "cf_12537",
  "entry_point": "logistic_regression",
  "mutant_count": 59,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=1.01, epochs=1000, threshold=0.5):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=1.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=-0.99, epochs=1000, threshold=0.5):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=-0.99, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=0, epochs=1000, threshold=0.5):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=1, epochs=1000, threshold=0.5):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=1, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=-0.01, epochs=1000, threshold=0.5):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=-0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1001, threshold=0.5):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1001, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=999, threshold=0.5):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=999, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=0, threshold=0.5):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=0, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1, threshold=0.5):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=-1000, threshold=0.5):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=-1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=1.5):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=1.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=-0.5):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=-0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=1):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=1):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):",
      "mutated_line": "def logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=-0.5):",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=-0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "bias = 0",
      "mutated_line": "bias = 1",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 1\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "bias = 0",
      "mutated_line": "bias = -1",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = -1\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 20,
      "original_line": "bias = 0",
      "mutated_line": "bias = 1",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 1\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "ASR",
      "lineno": 37,
      "original_line": "weights -= learning_rate * weights_derivative",
      "mutated_line": "weights += learning_rate * weights_derivative",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights += learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "ASR",
      "lineno": 38,
      "original_line": "bias -= learning_rate * bias_derivative",
      "mutated_line": "bias += learning_rate * bias_derivative",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias += learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 41,
      "original_line": "linear_combination = np.dot(X, weights) + bias",
      "mutated_line": "linear_combination = np.dot(X, weights) - bias",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) - bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 41,
      "original_line": "linear_combination = np.dot(X, weights) + bias",
      "mutated_line": "linear_combination = np.dot(X, weights) * bias",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) * bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 24,
      "original_line": "return 1 / (1 + np.exp(-x))",
      "mutated_line": "return 1 * (1 + np.exp(-x))",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 * (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 24,
      "original_line": "return 1 / (1 + np.exp(-x))",
      "mutated_line": "return 1 // (1 + np.exp(-x))",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 // (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 29,
      "original_line": "linear_combination = np.dot(X, weights) + bias",
      "mutated_line": "linear_combination = np.dot(X, weights) - bias",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) - bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 29,
      "original_line": "linear_combination = np.dot(X, weights) + bias",
      "mutated_line": "linear_combination = np.dot(X, weights) * bias",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) * bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "weights_derivative = np.dot(X.T, (probabilities - y)) / len(y)",
      "mutated_line": "weights_derivative = np.dot(X.T, probabilities - y) * len(y)",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) * len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "weights_derivative = np.dot(X.T, (probabilities - y)) / len(y)",
      "mutated_line": "weights_derivative = np.dot(X.T, probabilities - y) // len(y)",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) // len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 34,
      "original_line": "bias_derivative = np.sum(probabilities - y) / len(y)",
      "mutated_line": "bias_derivative = np.sum(probabilities - y) * len(y)",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) * len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 34,
      "original_line": "bias_derivative = np.sum(probabilities - y) / len(y)",
      "mutated_line": "bias_derivative = np.sum(probabilities - y) // len(y)",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) // len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 37,
      "original_line": "weights -= learning_rate * weights_derivative",
      "mutated_line": "weights -= learning_rate / weights_derivative",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate / weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 37,
      "original_line": "weights -= learning_rate * weights_derivative",
      "mutated_line": "weights -= learning_rate + weights_derivative",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate + weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 37,
      "original_line": "weights -= learning_rate * weights_derivative",
      "mutated_line": "weights -= learning_rate ** weights_derivative",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate ** weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 38,
      "original_line": "bias -= learning_rate * bias_derivative",
      "mutated_line": "bias -= learning_rate / bias_derivative",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate / bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 38,
      "original_line": "bias -= learning_rate * bias_derivative",
      "mutated_line": "bias -= learning_rate + bias_derivative",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate + bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 38,
      "original_line": "bias -= learning_rate * bias_derivative",
      "mutated_line": "bias -= learning_rate ** bias_derivative",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate ** bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "weights = np.zeros(X.shape[1])",
      "mutated_line": "weights = np.zeros(X.shape[2])",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[2])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "weights = np.zeros(X.shape[1])",
      "mutated_line": "weights = np.zeros(X.shape[0])",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[0])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "weights = np.zeros(X.shape[1])",
      "mutated_line": "weights = np.zeros(X.shape[0])",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[0])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "weights = np.zeros(X.shape[1])",
      "mutated_line": "weights = np.zeros(X.shape[-1])",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[-1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 24,
      "original_line": "return 1 / (1 + np.exp(-x))",
      "mutated_line": "return 2 / (1 + np.exp(-x))",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 2 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 24,
      "original_line": "return 1 / (1 + np.exp(-x))",
      "mutated_line": "return 0 / (1 + np.exp(-x))",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 0 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 24,
      "original_line": "return 1 / (1 + np.exp(-x))",
      "mutated_line": "return 0 / (1 + np.exp(-x))",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 0 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 24,
      "original_line": "return 1 / (1 + np.exp(-x))",
      "mutated_line": "return -1 / (1 + np.exp(-x))",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return -1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 24,
      "original_line": "return 1 / (1 + np.exp(-x))",
      "mutated_line": "return 1 / (1 - np.exp(-x))",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 - np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 24,
      "original_line": "return 1 / (1 + np.exp(-x))",
      "mutated_line": "return 1 / (1 * np.exp(-x))",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 * np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "ROR",
      "lineno": 45,
      "original_line": "predicted_class = (probabilities >= threshold).astype(int)",
      "mutated_line": "predicted_class = (probabilities > threshold).astype(int)",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities > threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "ROR",
      "lineno": 45,
      "original_line": "predicted_class = (probabilities >= threshold).astype(int)",
      "mutated_line": "predicted_class = (probabilities < threshold).astype(int)",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities < threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "ROR",
      "lineno": 45,
      "original_line": "predicted_class = (probabilities >= threshold).astype(int)",
      "mutated_line": "predicted_class = (probabilities == threshold).astype(int)",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities == threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 24,
      "original_line": "return 1 / (1 + np.exp(-x))",
      "mutated_line": "return 1 / (2 + np.exp(-x))",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (2 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 24,
      "original_line": "return 1 / (1 + np.exp(-x))",
      "mutated_line": "return 1 / (0 + np.exp(-x))",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (0 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 24,
      "original_line": "return 1 / (1 + np.exp(-x))",
      "mutated_line": "return 1 / (0 + np.exp(-x))",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (0 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "CRP",
      "lineno": 24,
      "original_line": "return 1 / (1 + np.exp(-x))",
      "mutated_line": "return 1 / (-1 + np.exp(-x))",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (-1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "weights_derivative = np.dot(X.T, (probabilities - y)) / len(y)",
      "mutated_line": "weights_derivative = np.dot(X.T, probabilities + y) / len(y)",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities + y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 33,
      "original_line": "weights_derivative = np.dot(X.T, (probabilities - y)) / len(y)",
      "mutated_line": "weights_derivative = np.dot(X.T, probabilities * y) / len(y)",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities * y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 34,
      "original_line": "bias_derivative = np.sum(probabilities - y) / len(y)",
      "mutated_line": "bias_derivative = np.sum(probabilities + y) / len(y)",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities + y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "AOR",
      "lineno": 34,
      "original_line": "bias_derivative = np.sum(probabilities - y) / len(y)",
      "mutated_line": "bias_derivative = np.sum(probabilities * y) / len(y)",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities * y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    },
    {
      "operator": "UOI",
      "lineno": 24,
      "original_line": "return 1 / (1 + np.exp(-x))",
      "mutated_line": "return 1 / (1 + np.exp(+x))",
      "code": "import numpy as np\n\ndef logistic_regression(X, y, learning_rate=0.01, epochs=1000, threshold=0.5):\n    \"\"\"\n    This function performs logistic regression from scratch in Python without using any external libraries.\n    \n    Parameters:\n    X (numpy array): Input data\n    y (numpy array): Labels\n    learning_rate (float): Learning rate for gradient descent (default is 0.01)\n    epochs (int): Number of iterations (default is 1000)\n    threshold (float): Threshold for binary classification (default is 0.5)\n    \n    Returns:\n    predicted_class: Predicted class labels\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(+x))\n    for _ in range(epochs):\n        linear_combination = np.dot(X, weights) + bias\n        probabilities = sigmoid(linear_combination)\n        weights_derivative = np.dot(X.T, probabilities - y) / len(y)\n        bias_derivative = np.sum(probabilities - y) / len(y)\n        weights -= learning_rate * weights_derivative\n        bias -= learning_rate * bias_derivative\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = sigmoid(linear_combination)\n    predicted_class = (probabilities >= threshold).astype(int)\n    return predicted_class"
    }
  ]
}