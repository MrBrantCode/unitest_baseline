{
  "task_id": "cf_5095",
  "entry_point": "linear_regression",
  "mutant_count": 49,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 4,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "ROR",
      "lineno": 18,
      "original_line": "if len(y.shape) == 1:",
      "mutated_line": "if len(y.shape) != 1:",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) != 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 18,
      "original_line": "if len(y.shape) == 1:",
      "mutated_line": "if len(y.shape) == 2:",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 2:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 18,
      "original_line": "if len(y.shape) == 1:",
      "mutated_line": "if len(y.shape) == 0:",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 0:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 18,
      "original_line": "if len(y.shape) == 1:",
      "mutated_line": "if len(y.shape) == 0:",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 0:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 18,
      "original_line": "if len(y.shape) == 1:",
      "mutated_line": "if len(y.shape) == -1:",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == -1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "n = X.shape[1]",
      "mutated_line": "n = X.shape[2]",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[2]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "n = X.shape[1]",
      "mutated_line": "n = X.shape[0]",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[0]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "n = X.shape[1]",
      "mutated_line": "n = X.shape[0]",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[0]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "n = X.shape[1]",
      "mutated_line": "n = X.shape[-1]",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[-1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 37,
      "original_line": "error = predictions - y",
      "mutated_line": "gradient = 1 / m * np.dot(X.T, error)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions + y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 37,
      "original_line": "error = predictions - y",
      "mutated_line": "gradient = 1 / m * np.dot(X.T, error)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions * y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 40,
      "original_line": "gradient = (1 / m) * np.dot(X.T, error)",
      "mutated_line": "gradient = 1 / m / np.dot(X.T, error)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m / np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 40,
      "original_line": "gradient = (1 / m) * np.dot(X.T, error)",
      "mutated_line": "gradient = 1 / m + np.dot(X.T, error)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m + np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 40,
      "original_line": "gradient = (1 / m) * np.dot(X.T, error)",
      "mutated_line": "gradient = (1 / m) ** np.dot(X.T, error)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = (1 / m) ** np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 43,
      "original_line": "theta = theta - alpha * gradient",
      "mutated_line": "theta = theta + alpha * gradient",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta + alpha * gradient\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 43,
      "original_line": "theta = theta - alpha * gradient",
      "mutated_line": "theta = theta * (alpha * gradient)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta * (alpha * gradient)\n    return theta"
    },
    {
      "operator": "UOI",
      "lineno": 19,
      "original_line": "y = y.reshape(-1, 1)",
      "mutated_line": "y = y.reshape(+1, 1)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(+1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "y = y.reshape(-1, 1)",
      "mutated_line": "y = y.reshape(-1, 2)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 2)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "y = y.reshape(-1, 1)",
      "mutated_line": "y = y.reshape(-1, 0)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 0)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "y = y.reshape(-1, 1)",
      "mutated_line": "y = y.reshape(-1, 0)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 0)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "y = y.reshape(-1, 1)",
      "mutated_line": "y = y.reshape(-1, -1)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, -1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 26,
      "original_line": "theta = np.zeros((n + 1, 1))",
      "mutated_line": "theta = np.zeros((n - 1, 1))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n - 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 26,
      "original_line": "theta = np.zeros((n + 1, 1))",
      "mutated_line": "theta = np.zeros((n * 1, 1))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n * 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "theta = np.zeros((n + 1, 1))",
      "mutated_line": "theta = np.zeros((n + 1, 2))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 2))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "theta = np.zeros((n + 1, 1))",
      "mutated_line": "theta = np.zeros((n + 1, 0))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 0))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "theta = np.zeros((n + 1, 1))",
      "mutated_line": "theta = np.zeros((n + 1, 0))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 0))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "theta = np.zeros((n + 1, 1))",
      "mutated_line": "theta = np.zeros((n + 1, -1))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, -1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 40,
      "original_line": "gradient = (1 / m) * np.dot(X.T, error)",
      "mutated_line": "gradient = 1 * m * np.dot(X.T, error)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 * m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 40,
      "original_line": "gradient = (1 / m) * np.dot(X.T, error)",
      "mutated_line": "gradient = 1 // m * np.dot(X.T, error)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 // m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 43,
      "original_line": "theta = theta - alpha * gradient",
      "mutated_line": "theta = theta - alpha / gradient",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha / gradient\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 43,
      "original_line": "theta = theta - alpha * gradient",
      "mutated_line": "theta = theta - (alpha + gradient)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - (alpha + gradient)\n    return theta"
    },
    {
      "operator": "AOR",
      "lineno": 43,
      "original_line": "theta = theta - alpha * gradient",
      "mutated_line": "theta = theta - alpha ** gradient",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha ** gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "y = y.reshape(-1, 1)",
      "mutated_line": "y = y.reshape(-2, 1)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-2, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "y = y.reshape(-1, 1)",
      "mutated_line": "y = y.reshape(-0, 1)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-0, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "y = y.reshape(-1, 1)",
      "mutated_line": "y = y.reshape(-0, 1)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-0, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "y = y.reshape(-1, 1)",
      "mutated_line": "y = y.reshape(--1, 1)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(--1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "theta = np.zeros((n + 1, 1))",
      "mutated_line": "theta = np.zeros((n + 2, 1))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 2, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "theta = np.zeros((n + 1, 1))",
      "mutated_line": "theta = np.zeros((n + 0, 1))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 0, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "theta = np.zeros((n + 1, 1))",
      "mutated_line": "theta = np.zeros((n + 0, 1))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 0, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "theta = np.zeros((n + 1, 1))",
      "mutated_line": "theta = np.zeros((n + -1, 1))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + -1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 40,
      "original_line": "gradient = (1 / m) * np.dot(X.T, error)",
      "mutated_line": "gradient = 2 / m * np.dot(X.T, error)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 2 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 40,
      "original_line": "gradient = (1 / m) * np.dot(X.T, error)",
      "mutated_line": "gradient = 0 / m * np.dot(X.T, error)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 0 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 40,
      "original_line": "gradient = (1 / m) * np.dot(X.T, error)",
      "mutated_line": "gradient = 0 / m * np.dot(X.T, error)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 0 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 40,
      "original_line": "gradient = (1 / m) * np.dot(X.T, error)",
      "mutated_line": "gradient = -1 / m * np.dot(X.T, error)",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = -1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 29,
      "original_line": "X = np.hstack((np.ones((m, 1)), X))",
      "mutated_line": "X = np.hstack((np.ones((m, 2)), X))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 2)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 29,
      "original_line": "X = np.hstack((np.ones((m, 1)), X))",
      "mutated_line": "X = np.hstack((np.ones((m, 0)), X))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 0)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 29,
      "original_line": "X = np.hstack((np.ones((m, 1)), X))",
      "mutated_line": "X = np.hstack((np.ones((m, 0)), X))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, 0)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    },
    {
      "operator": "CRP",
      "lineno": 29,
      "original_line": "X = np.hstack((np.ones((m, 1)), X))",
      "mutated_line": "X = np.hstack((np.ones((m, -1)), X))",
      "code": "import numpy as np\n\ndef linear_regression(X, y, alpha, num_iterations):\n    \"\"\"\n    This function implements linear regression using gradient descent.\n\n    Parameters:\n    X (numpy array): A 2D numpy array of independent variables.\n    y (numpy array): A 1D or 2D numpy array of dependent variables.\n    alpha (float): The learning rate for gradient descent.\n    num_iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    numpy array: The optimized coefficients (including the intercept term) of the linear regression model.\n    \"\"\"\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros((n + 1, 1))\n    X = np.hstack((np.ones((m, -1)), X))\n    for _ in range(num_iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = 1 / m * np.dot(X.T, error)\n        theta = theta - alpha * gradient\n    return theta"
    }
  ]
}