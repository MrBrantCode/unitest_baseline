{
  "task_id": "cf_61855",
  "entry_point": "hadoop_data_skew_mitigation",
  "mutant_count": 8,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 2,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "def hadoop_data_skew_mitigation(data_chunk_sizes, num_nodes):\n    \"\"\"\"\"\"\n    data_chunk_sizes.sort(reverse=True)\n    data_distribution = {i: 0 for i in range(num_nodes)}\n    for (i, chunk_size) in enumerate(data_chunk_sizes):\n        node_id = i % num_nodes\n        data_distribution[node_id] += chunk_size\n    return data_distribution"
    },
    {
      "operator": "ASR",
      "lineno": 22,
      "original_line": "data_distribution[node_id] += chunk_size",
      "mutated_line": "data_distribution[node_id] -= chunk_size",
      "code": "def hadoop_data_skew_mitigation(data_chunk_sizes, num_nodes):\n    \"\"\"\n    Mitigate the impact of data skew in Hadoop MapReduce jobs by redistributing data chunks across nodes.\n    \n    Args:\n    data_chunk_sizes (list): A list of integers representing the sizes of data chunks.\n    num_nodes (int): The number of nodes in the cluster.\n    \n    Returns:\n    dict: A dictionary containing the mitigated data distribution across nodes in the cluster.\n    \"\"\"\n    data_chunk_sizes.sort(reverse=True)\n    data_distribution = {i: 0 for i in range(num_nodes)}\n    for (i, chunk_size) in enumerate(data_chunk_sizes):\n        node_id = i % num_nodes\n        data_distribution[node_id] -= chunk_size\n    return data_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "data_distribution = {i: 0 for i in range(num_nodes)}",
      "mutated_line": "for (i, chunk_size) in enumerate(data_chunk_sizes):",
      "code": "def hadoop_data_skew_mitigation(data_chunk_sizes, num_nodes):\n    \"\"\"\n    Mitigate the impact of data skew in Hadoop MapReduce jobs by redistributing data chunks across nodes.\n    \n    Args:\n    data_chunk_sizes (list): A list of integers representing the sizes of data chunks.\n    num_nodes (int): The number of nodes in the cluster.\n    \n    Returns:\n    dict: A dictionary containing the mitigated data distribution across nodes in the cluster.\n    \"\"\"\n    data_chunk_sizes.sort(reverse=True)\n    data_distribution = {i: 1 for i in range(num_nodes)}\n    for (i, chunk_size) in enumerate(data_chunk_sizes):\n        node_id = i % num_nodes\n        data_distribution[node_id] += chunk_size\n    return data_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "data_distribution = {i: 0 for i in range(num_nodes)}",
      "mutated_line": "for (i, chunk_size) in enumerate(data_chunk_sizes):",
      "code": "def hadoop_data_skew_mitigation(data_chunk_sizes, num_nodes):\n    \"\"\"\n    Mitigate the impact of data skew in Hadoop MapReduce jobs by redistributing data chunks across nodes.\n    \n    Args:\n    data_chunk_sizes (list): A list of integers representing the sizes of data chunks.\n    num_nodes (int): The number of nodes in the cluster.\n    \n    Returns:\n    dict: A dictionary containing the mitigated data distribution across nodes in the cluster.\n    \"\"\"\n    data_chunk_sizes.sort(reverse=True)\n    data_distribution = {i: -1 for i in range(num_nodes)}\n    for (i, chunk_size) in enumerate(data_chunk_sizes):\n        node_id = i % num_nodes\n        data_distribution[node_id] += chunk_size\n    return data_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 17,
      "original_line": "data_distribution = {i: 0 for i in range(num_nodes)}",
      "mutated_line": "for (i, chunk_size) in enumerate(data_chunk_sizes):",
      "code": "def hadoop_data_skew_mitigation(data_chunk_sizes, num_nodes):\n    \"\"\"\n    Mitigate the impact of data skew in Hadoop MapReduce jobs by redistributing data chunks across nodes.\n    \n    Args:\n    data_chunk_sizes (list): A list of integers representing the sizes of data chunks.\n    num_nodes (int): The number of nodes in the cluster.\n    \n    Returns:\n    dict: A dictionary containing the mitigated data distribution across nodes in the cluster.\n    \"\"\"\n    data_chunk_sizes.sort(reverse=True)\n    data_distribution = {i: 1 for i in range(num_nodes)}\n    for (i, chunk_size) in enumerate(data_chunk_sizes):\n        node_id = i % num_nodes\n        data_distribution[node_id] += chunk_size\n    return data_distribution"
    },
    {
      "operator": "AOR",
      "lineno": 21,
      "original_line": "node_id = i % num_nodes",
      "mutated_line": "node_id = i * num_nodes",
      "code": "def hadoop_data_skew_mitigation(data_chunk_sizes, num_nodes):\n    \"\"\"\n    Mitigate the impact of data skew in Hadoop MapReduce jobs by redistributing data chunks across nodes.\n    \n    Args:\n    data_chunk_sizes (list): A list of integers representing the sizes of data chunks.\n    num_nodes (int): The number of nodes in the cluster.\n    \n    Returns:\n    dict: A dictionary containing the mitigated data distribution across nodes in the cluster.\n    \"\"\"\n    data_chunk_sizes.sort(reverse=True)\n    data_distribution = {i: 0 for i in range(num_nodes)}\n    for (i, chunk_size) in enumerate(data_chunk_sizes):\n        node_id = i * num_nodes\n        data_distribution[node_id] += chunk_size\n    return data_distribution"
    },
    {
      "operator": "AOR",
      "lineno": 21,
      "original_line": "node_id = i % num_nodes",
      "mutated_line": "node_id = i + num_nodes",
      "code": "def hadoop_data_skew_mitigation(data_chunk_sizes, num_nodes):\n    \"\"\"\n    Mitigate the impact of data skew in Hadoop MapReduce jobs by redistributing data chunks across nodes.\n    \n    Args:\n    data_chunk_sizes (list): A list of integers representing the sizes of data chunks.\n    num_nodes (int): The number of nodes in the cluster.\n    \n    Returns:\n    dict: A dictionary containing the mitigated data distribution across nodes in the cluster.\n    \"\"\"\n    data_chunk_sizes.sort(reverse=True)\n    data_distribution = {i: 0 for i in range(num_nodes)}\n    for (i, chunk_size) in enumerate(data_chunk_sizes):\n        node_id = i + num_nodes\n        data_distribution[node_id] += chunk_size\n    return data_distribution"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "data_chunk_sizes.sort(reverse=True)",
      "mutated_line": "data_chunk_sizes.sort(reverse=False)",
      "code": "def hadoop_data_skew_mitigation(data_chunk_sizes, num_nodes):\n    \"\"\"\n    Mitigate the impact of data skew in Hadoop MapReduce jobs by redistributing data chunks across nodes.\n    \n    Args:\n    data_chunk_sizes (list): A list of integers representing the sizes of data chunks.\n    num_nodes (int): The number of nodes in the cluster.\n    \n    Returns:\n    dict: A dictionary containing the mitigated data distribution across nodes in the cluster.\n    \"\"\"\n    data_chunk_sizes.sort(reverse=False)\n    data_distribution = {i: 0 for i in range(num_nodes)}\n    for (i, chunk_size) in enumerate(data_chunk_sizes):\n        node_id = i % num_nodes\n        data_distribution[node_id] += chunk_size\n    return data_distribution"
    }
  ]
}