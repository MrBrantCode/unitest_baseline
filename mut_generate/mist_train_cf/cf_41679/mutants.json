{
  "task_id": "cf_41679",
  "entry_point": "calculate_precision_recall",
  "mutant_count": 129,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 2,
      "original_line": "\"\"\"",
      "mutated_line": "\"\"\"\"\"\"",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 28,
      "original_line": "macro_precision = sum(precision_per_class) / len(precision_per_class)",
      "mutated_line": "macro_precision = sum(precision_per_class) * len(precision_per_class)",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) * len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 28,
      "original_line": "macro_precision = sum(precision_per_class) / len(precision_per_class)",
      "mutated_line": "macro_precision = sum(precision_per_class) // len(precision_per_class)",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) // len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 29,
      "original_line": "macro_recall = sum(recall_per_class) / len(recall_per_class)",
      "mutated_line": "macro_recall = sum(recall_per_class) * len(recall_per_class)",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) * len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 29,
      "original_line": "macro_recall = sum(recall_per_class) / len(recall_per_class)",
      "mutated_line": "macro_recall = sum(recall_per_class) // len(recall_per_class)",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) // len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives >= 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives <= 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives * (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives // (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 1\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else -1\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 1\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives >= 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives <= 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives * (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives // (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 1\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else -1\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 1\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((2 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((2 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((0 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((0 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((0 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((0 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((-1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((-1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((2 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((2 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((0 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((0 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((0 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((0 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((-1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((-1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((2 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((2 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((0 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((0 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((0 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((0 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((-1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((-1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives - false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives * false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 1 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > -1 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 1 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives - false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 18,
      "original_line": "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0",
      "mutated_line": "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives * false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives - false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives * false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 1 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > -1 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 1 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives - false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 19,
      "original_line": "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0",
      "mutated_line": "true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives * false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp >= 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp <= 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp != 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp * (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp // (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 1 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else -1 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 1 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn >= 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn >= 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn <= 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn <= 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn != 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn != 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp * (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp * (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp // (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp // (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 1 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 1 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else -1 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else -1 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 1 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 1 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "LCR",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 or l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 or l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "LCR",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 or l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 or l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "LCR",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 or l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 or l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 21,
      "original_line": "true_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l == c) for c in set(labels)]",
      "mutated_line": "false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((2 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 21,
      "original_line": "true_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l == c) for c in set(labels)]",
      "mutated_line": "false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((0 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 21,
      "original_line": "true_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l == c) for c in set(labels)]",
      "mutated_line": "false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((0 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 21,
      "original_line": "true_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l == c) for c in set(labels)]",
      "mutated_line": "false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((-1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 22,
      "original_line": "false_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l != c) for c in set(labels)]",
      "mutated_line": "precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((2 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 22,
      "original_line": "false_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l != c) for c in set(labels)]",
      "mutated_line": "precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((0 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 22,
      "original_line": "false_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l != c) for c in set(labels)]",
      "mutated_line": "precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((0 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 22,
      "original_line": "false_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l != c) for c in set(labels)]",
      "mutated_line": "precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((-1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "false_negatives_per_class = [sum(1 for p, l in zip(predictions, labels) if p != c and l == c) for c in set(labels)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((2 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "false_negatives_per_class = [sum(1 for p, l in zip(predictions, labels) if p != c and l == c) for c in set(labels)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((0 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "false_negatives_per_class = [sum(1 for p, l in zip(predictions, labels) if p != c and l == c) for c in set(labels)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((0 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 23,
      "original_line": "false_negatives_per_class = [sum(1 for p, l in zip(predictions, labels) if p != c and l == c) for c in set(labels)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((-1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp - fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp * fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 1 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > -1 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 1 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp - fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 25,
      "original_line": "precision_per_class = [tp / (tp + fp) if (tp + fp) > 0 else 0 for tp, fp in zip(true_positives_per_class, false_positives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp * fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp - fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp - fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp * fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp * fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 1 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 1 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > -1 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > -1 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 1 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 1 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp - fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp - fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "AOR",
      "lineno": 26,
      "original_line": "recall_per_class = [tp / (tp + fn) if (tp + fn) > 0 else 0 for tp, fn in zip(true_positives_per_class, false_negatives_per_class)]",
      "mutated_line": "recall_per_class = [tp / (tp * fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp * fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((1 for (p, l) in zip(predictions, labels) if p != 1 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p != 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l != 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l != 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((1 for (p, l) in zip(predictions, labels) if p != 1 and l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p != 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l != 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l != 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p != 0 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p != 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l != 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l != 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "LCR",
      "lineno": 21,
      "original_line": "true_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l == c) for c in set(labels)]",
      "mutated_line": "false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c or l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "LCR",
      "lineno": 22,
      "original_line": "false_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l != c) for c in set(labels)]",
      "mutated_line": "precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c or l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "LCR",
      "lineno": 23,
      "original_line": "false_negatives_per_class = [sum(1 for p, l in zip(predictions, labels) if p != c and l == c) for c in set(labels)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c or l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 2 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 2 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == -1 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == -1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 2))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 2))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 14,
      "original_line": "true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)",
      "mutated_line": "true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == -1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == -1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 2 and l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 2 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == -1 and l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == -1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == -1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == -1))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)",
      "mutated_line": "false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == -1 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == -1 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 2))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 2))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 0))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 0))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 0))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "CRP",
      "lineno": 16,
      "original_line": "false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)",
      "mutated_line": "false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == -1))",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == -1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 21,
      "original_line": "true_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l == c) for c in set(labels)]",
      "mutated_line": "false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 21,
      "original_line": "true_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l == c) for c in set(labels)]",
      "mutated_line": "false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 22,
      "original_line": "false_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l != c) for c in set(labels)]",
      "mutated_line": "precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 22,
      "original_line": "false_positives_per_class = [sum(1 for p, l in zip(predictions, labels) if p == c and l != c) for c in set(labels)]",
      "mutated_line": "precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 23,
      "original_line": "false_negatives_per_class = [sum(1 for p, l in zip(predictions, labels) if p != c and l == c) for c in set(labels)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    },
    {
      "operator": "ROR",
      "lineno": 23,
      "original_line": "false_negatives_per_class = [sum(1 for p, l in zip(predictions, labels) if p != c and l == c) for c in set(labels)]",
      "mutated_line": "recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]",
      "code": "def calculate_precision_recall(predictions, labels):\n    \"\"\"\n    Calculate precision and recall metrics on the entity level, as well as the 'macro' average on well-defined classes.\n\n    Args:\n    predictions (list): List of predicted labels for each instance.\n    labels (list): List of true labels for each instance.\n\n    Returns:\n    precision (float): Precision metric.\n    recall (float): Recall metric.\n    macro_avg (tuple): 'Macro' average on well-defined classes as a tuple of macro precision and macro recall.\n    \"\"\"\n    true_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 1))\n    false_positives = sum((1 for (p, l) in zip(predictions, labels) if p == 1 and l == 0))\n    false_negatives = sum((1 for (p, l) in zip(predictions, labels) if p == 0 and l == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    true_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l == c)) for c in set(labels)]\n    false_positives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p == c and l != c)) for c in set(labels)]\n    false_negatives_per_class = [sum((1 for (p, l) in zip(predictions, labels) if p != c and l != c)) for c in set(labels)]\n    precision_per_class = [tp / (tp + fp) if tp + fp > 0 else 0 for (tp, fp) in zip(true_positives_per_class, false_positives_per_class)]\n    recall_per_class = [tp / (tp + fn) if tp + fn > 0 else 0 for (tp, fn) in zip(true_positives_per_class, false_negatives_per_class)]\n    macro_precision = sum(precision_per_class) / len(precision_per_class)\n    macro_recall = sum(recall_per_class) / len(recall_per_class)\n    return (precision, recall, (macro_precision, macro_recall))"
    }
  ]
}