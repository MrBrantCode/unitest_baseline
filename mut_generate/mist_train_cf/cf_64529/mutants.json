{
  "task_id": "cf_64529",
  "entry_point": "compress_textual_data",
  "mutant_count": 2,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "tokens = set([word for sentence in data for word in sentence.split(\" \")])",
      "mutated_line": "token_map = {token: i for (i, token) in enumerate(tokens)}",
      "code": "def compress_textual_data(data):\n    tokens = set([word for sentence in data for word in sentence.split('')])\n    token_map = {token: i for (i, token) in enumerate(tokens)}\n    compressed_data = [[token_map[word] for word in sentence.split(' ')] for sentence in data]\n    return (compressed_data, token_map)"
    },
    {
      "operator": "CRP",
      "lineno": 9,
      "original_line": "compressed_data = [[token_map[word] for word in sentence.split(\" \")] for sentence in data]",
      "mutated_line": "return (compressed_data, token_map)",
      "code": "def compress_textual_data(data):\n    tokens = set([word for sentence in data for word in sentence.split(' ')])\n    token_map = {token: i for (i, token) in enumerate(tokens)}\n    compressed_data = [[token_map[word] for word in sentence.split('')] for sentence in data]\n    return (compressed_data, token_map)"
    }
  ]
}