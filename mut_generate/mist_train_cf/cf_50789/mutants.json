{
  "task_id": "cf_50789",
  "entry_point": "gradient_descent",
  "mutant_count": 48,
  "mutants": [
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=1.1, max_iters=10000, precision=0.0001):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=1.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=-0.9, max_iters=10000, precision=0.0001):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=-0.9, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0, max_iters=10000, precision=0.0001):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=1, max_iters=10000, precision=0.0001):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=-0.1, max_iters=10000, precision=0.0001):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=-0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10001, precision=0.0001):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10001, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=9999, precision=0.0001):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=9999, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=0, precision=0.0001):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=0, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=1, precision=0.0001):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=1, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=-10000, precision=0.0001):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=-10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=1.0001):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=1.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=-0.9999):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=-0.9999):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=1):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=1):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 3,
      "original_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):",
      "mutated_line": "def gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=-0.0001):",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=-0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "iter_counter = 0",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 1\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "iter_counter = 0",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = -1\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 7,
      "original_line": "iter_counter = 0",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 1\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "LCR",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision ** 2 or iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 or iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "ASR",
      "lineno": 15,
      "original_line": "iter_counter += 1",
      "mutated_line": "iter_counter -= 1",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter -= 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 5,
      "original_line": "step_sizes = [1]",
      "mutated_line": "step_sizes = [2]",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [2]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 5,
      "original_line": "step_sizes = [1]",
      "mutated_line": "step_sizes = [0]",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [0]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 5,
      "original_line": "step_sizes = [1]",
      "mutated_line": "step_sizes = [0]",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [0]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 5,
      "original_line": "step_sizes = [1]",
      "mutated_line": "step_sizes = [-1]",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [-1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "ROR",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) >= precision ** 2 and iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) >= precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "ROR",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) <= precision ** 2 and iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) <= precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "ROR",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) != precision ** 2 and iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) != precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "ROR",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter <= max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter <= max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "ROR",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter >= max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter >= max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "ROR",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter != max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter != max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "AOR",
      "lineno": 10,
      "original_line": "current_point = prev_point - learning_rate * derivative(prev_point)",
      "mutated_line": "current_point = prev_point + learning_rate * derivative(prev_point)",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point + learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "AOR",
      "lineno": 10,
      "original_line": "current_point = prev_point - learning_rate * derivative(prev_point)",
      "mutated_line": "current_point = prev_point * (learning_rate * derivative(prev_point))",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point * (learning_rate * derivative(prev_point))\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "AOR",
      "lineno": 13,
      "original_line": "step_sizes = current_point - prev_point",
      "mutated_line": "step_sizes = current_point + prev_point",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point + prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "AOR",
      "lineno": 13,
      "original_line": "step_sizes = current_point - prev_point",
      "mutated_line": "step_sizes = current_point * prev_point",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point * prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "iter_counter += 1",
      "mutated_line": "iter_counter += 2",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 2\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "iter_counter += 1",
      "mutated_line": "iter_counter += 0",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 0\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "iter_counter += 1",
      "mutated_line": "iter_counter += 0",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 0\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 15,
      "original_line": "iter_counter += 1",
      "mutated_line": "iter_counter += -1",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += -1\n    return current_point"
    },
    {
      "operator": "AOR",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision * 2 and iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision * 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "AOR",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision + 2 and iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision + 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "AOR",
      "lineno": 10,
      "original_line": "current_point = prev_point - learning_rate * derivative(prev_point)",
      "mutated_line": "current_point = prev_point - learning_rate / derivative(prev_point)",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate / derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "AOR",
      "lineno": 10,
      "original_line": "current_point = prev_point - learning_rate * derivative(prev_point)",
      "mutated_line": "current_point = prev_point - (learning_rate + derivative(prev_point))",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - (learning_rate + derivative(prev_point))\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "AOR",
      "lineno": 10,
      "original_line": "current_point = prev_point - learning_rate * derivative(prev_point)",
      "mutated_line": "current_point = prev_point - learning_rate ** derivative(prev_point)",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate ** derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision ** 3 and iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 3 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision ** 1 and iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 1 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision ** 0 and iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 0 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision ** 1 and iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** 1 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    },
    {
      "operator": "CRP",
      "lineno": 8,
      "original_line": "while np.sum(np.square(step_sizes)) > precision**2 and iter_counter < max_iters:",
      "mutated_line": "while np.sum(np.square(step_sizes)) > precision ** -2 and iter_counter < max_iters:",
      "code": "import numpy as np\n\ndef gradient_descent(func, derivative, initial_point, learning_rate=0.1, max_iters=10000, precision=0.0001):\n    current_point = initial_point\n    step_sizes = [1]\n    iter_counter = 0\n    while np.sum(np.square(step_sizes)) > precision ** -2 and iter_counter < max_iters:\n        prev_point = current_point\n        current_point = prev_point - learning_rate * derivative(prev_point)\n        step_sizes = current_point - prev_point\n        iter_counter += 1\n    return current_point"
    }
  ]
}